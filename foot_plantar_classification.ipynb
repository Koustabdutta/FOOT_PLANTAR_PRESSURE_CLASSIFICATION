{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b80bc45d-35ef-485b-8947-2ff70c08731d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Professional Plantar Pressure Analysis System\n",
      "============================================================\n",
      "Device: cuda\n",
      "Dataset: C:\\Users\\koust\\AnaKonda\\FOOT_PLANTAR_CLASSIFICATION\\Dataset\n",
      "============================================================\n",
      "Loaded 202 existing labels\n",
      "Extracting features from 202 images...\n",
      "Extracted features from 202 images\n",
      "Features saved to features\\extracted_features.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"C:\\Users\\koust\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "        \"wmic CPU Get NumberOfCores /Format:csv\".split(),\n",
      "        capture_output=True,\n",
      "        text=True,\n",
      "    )\n",
      "  File \"C:\\Users\\koust\\anaconda3\\Lib\\subprocess.py\", line 554, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\koust\\anaconda3\\Lib\\subprocess.py\", line 1039, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        pass_fds, cwd, env,\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "                        gid, gids, uid, umask,\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        start_new_session, process_group)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\koust\\anaconda3\\Lib\\subprocess.py\", line 1554, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "                             # no special security\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "                             cwd,\n",
      "                             ^^^^\n",
      "                             startupinfo)\n",
      "                             ^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.160\n",
      "Clustering results saved to results\\clustering_kmeans_results.csv\n",
      "Visualization saved to results\\clustering_visualization.png\n",
      "\n",
      "Training on cuda\n",
      "Train size: 161\n",
      "Val size: 41\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Professional Plantar Pressure Analysis System\n",
    "==============================================\n",
    "\n",
    "This system provides three approaches for analyzing plantar pressure maps:\n",
    "1. Supervised Learning (when labels are available)\n",
    "2. Unsupervised Learning (anomaly detection & clustering)\n",
    "3. Interactive Labeling Tool\n",
    "\n",
    "Author: Research Project\n",
    "Date: 2025\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import threading\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image, ImageTk\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox, ttk\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Compatibility for Pillow resampling\n",
    "try:\n",
    "    RESAMPLE_LANCZOS = Image.Resampling.LANCZOS\n",
    "except Exception:\n",
    "    RESAMPLE_LANCZOS = Image.LANCZOS\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Centralized configuration management\"\"\"\n",
    "    \n",
    "    # Paths\n",
    "    DATASET_DIR = r\"C:\\Users\\koust\\AnaKonda\\FOOT_PLANTAR_CLASSIFICATION\\Dataset\"\n",
    "    LABELS_CSV = \"plantar_labels.csv\"\n",
    "    MODEL_SAVE_PATH = \"models/plantar_model.pth\"\n",
    "    RESULTS_DIR = \"results\"\n",
    "    FEATURES_DIR = \"features\"\n",
    "    \n",
    "    # Model settings\n",
    "    IMAGE_SIZE = (224, 224)  # Standard ImageNet size\n",
    "    BATCH_SIZE = 16\n",
    "    EPOCHS = 100\n",
    "    LEARNING_RATE = 0.0001\n",
    "    EARLY_STOPPING_PATIENCE = 15\n",
    "    \n",
    "    # Device configuration\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Feature extraction\n",
    "    FEATURE_METHODS = ['pressure_stats', 'pressure_distribution', 'asymmetry', 'cog']\n",
    "    \n",
    "    # Clustering\n",
    "    N_CLUSTERS = 3  # Can be adjusted\n",
    "    \n",
    "    @classmethod\n",
    "    def create_directories(cls):\n",
    "        \"\"\"Create necessary directories\"\"\"\n",
    "        for dir_path in [cls.RESULTS_DIR, cls.FEATURES_DIR, 'models', 'logs']:\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE EXTRACTION\n",
    "# =============================================================================\n",
    "\n",
    "class PressureMapFeatureExtractor:\n",
    "    \"\"\"Extract meaningful features from plantar pressure maps\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_names = []\n",
    "    \n",
    "    def extract_all_features(self, image_path):\n",
    "        \"\"\"Extract comprehensive features from pressure map\"\"\"\n",
    "        try:\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Could not load image: {image_path}\")\n",
    "            \n",
    "            # Convert to grayscale for pressure analysis\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            features = {}\n",
    "            features.update(self._extract_pressure_statistics(gray))\n",
    "            features.update(self._extract_pressure_distribution(gray))\n",
    "            features.update(self._extract_spatial_features(gray))\n",
    "            features.update(self._extract_asymmetry_features(image))\n",
    "            \n",
    "            return features\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting features from {image_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _extract_pressure_statistics(self, gray_image):\n",
    "        \"\"\"Basic statistical features of pressure\"\"\"\n",
    "        features = {\n",
    "            'mean_pressure': float(np.mean(gray_image)),\n",
    "            'std_pressure': float(np.std(gray_image)),\n",
    "            'max_pressure': float(np.max(gray_image)),\n",
    "            'min_pressure': float(np.min(gray_image)),\n",
    "            'median_pressure': float(np.median(gray_image)),\n",
    "            'pressure_range': float(np.max(gray_image) - np.min(gray_image)),\n",
    "        }\n",
    "        return features\n",
    "    \n",
    "    def _extract_pressure_distribution(self, gray_image):\n",
    "        \"\"\"Pressure distribution features\"\"\"\n",
    "        hist, _ = np.histogram(gray_image, bins=10, range=(0, 255))\n",
    "        hist = hist.astype(float)\n",
    "        if hist.sum() > 0:\n",
    "            hist = hist / hist.sum()  # Normalize\n",
    "        else:\n",
    "            hist = np.zeros_like(hist, dtype=float)\n",
    "        \n",
    "        features = {f'pressure_bin_{i}': float(val) for i, val in enumerate(hist)}\n",
    "        \n",
    "        # High pressure area\n",
    "        high_pressure_threshold = np.percentile(gray_image, 75)\n",
    "        high_pressure_area = float(np.sum(gray_image > high_pressure_threshold) / gray_image.size)\n",
    "        features['high_pressure_area_ratio'] = high_pressure_area\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _extract_spatial_features(self, gray_image):\n",
    "        \"\"\"Spatial distribution features\"\"\"\n",
    "        # Center of gravity\n",
    "        y_coords, x_coords = np.indices(gray_image.shape)\n",
    "        total_pressure = float(np.sum(gray_image))\n",
    "        \n",
    "        if total_pressure > 0:\n",
    "            cog_x = float(np.sum(x_coords * gray_image) / total_pressure)\n",
    "            cog_y = float(np.sum(y_coords * gray_image) / total_pressure)\n",
    "        else:\n",
    "            cog_x, cog_y = float(gray_image.shape[1] / 2), float(gray_image.shape[0] / 2)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        features = {\n",
    "            'cog_x_normalized': float(cog_x / gray_image.shape[1]),\n",
    "            'cog_y_normalized': float(cog_y / gray_image.shape[0]),\n",
    "        }\n",
    "        \n",
    "        # Contact area\n",
    "        contact_threshold = float(np.mean(gray_image) + np.std(gray_image))\n",
    "        contact_area = float(np.sum(gray_image > contact_threshold) / gray_image.size)\n",
    "        features['contact_area_ratio'] = contact_area\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _extract_asymmetry_features(self, image):\n",
    "        \"\"\"Extract left-right asymmetry features\"\"\"\n",
    "        h, w = image.shape[:2]\n",
    "        left_half = image[:, :w//2]\n",
    "        right_half = image[:, w//2:]\n",
    "        \n",
    "        # Flip right half for comparison\n",
    "        right_flipped = cv2.flip(right_half, 1)\n",
    "        \n",
    "        # Ensure same size\n",
    "        min_width = min(left_half.shape[1], right_flipped.shape[1])\n",
    "        left_half = left_half[:, :min_width]\n",
    "        right_flipped = right_flipped[:, :min_width]\n",
    "        \n",
    "        # Calculate asymmetry\n",
    "        diff = cv2.absdiff(left_half, right_flipped)\n",
    "        asymmetry_score = float(np.mean(diff) / 255.0) if diff.size else 0.0\n",
    "        \n",
    "        features = {\n",
    "            'lr_asymmetry': asymmetry_score,\n",
    "            'left_mean_pressure': float(np.mean(left_half)) if left_half.size else 0.0,\n",
    "            'right_mean_pressure': float(np.mean(right_half)) if right_half.size else 0.0,\n",
    "        }\n",
    "        \n",
    "        return features\n",
    "\n",
    "# =============================================================================\n",
    "# ADVANCED CNN MODEL\n",
    "# =============================================================================\n",
    "\n",
    "class ImprovedPlantarCNN(nn.Module):\n",
    "    \"\"\"Professional CNN with transfer learning and attention\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=2, pretrained=True):\n",
    "        super(ImprovedPlantarCNN, self).__init__()\n",
    "        \n",
    "        # Use ResNet18 as backbone\n",
    "        self.backbone = models.resnet18(pretrained=pretrained)\n",
    "        \n",
    "        # Modify first conv layer to accept our input\n",
    "        self.backbone.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, \n",
    "                                        padding=3, bias=False)\n",
    "        \n",
    "        # Get feature dimension\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        \n",
    "        # Replace final layer with custom classifier\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# =============================================================================\n",
    "# DATASET WITH AUGMENTATION\n",
    "# =============================================================================\n",
    "\n",
    "class PlantarDataset(Dataset):\n",
    "    \"\"\"Enhanced dataset with proper augmentation\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, transform=None, augment=False):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        self.label_map = {'healthy': 0, 'unhealthy': 1, 'normal': 0, 'abnormal': 1}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx]['image_path']\n",
    "        label_str = self.df.iloc[idx]['label']\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            image = Image.new('RGB', Config.IMAGE_SIZE, color='black')\n",
    "            label_str = 'healthy'\n",
    "        \n",
    "        label = self.label_map.get(str(label_str).lower(), 0)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label, img_path\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING ENGINE (backwards-compatible scheduler)\n",
    "# =============================================================================\n",
    "\n",
    "class TrainingEngine:\n",
    "    \"\"\"Professional training with all best practices (backwards-compatible scheduler)\"\"\"\n",
    "    \n",
    "    def __init__(self, model, train_loader, val_loader, device):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        \n",
    "        # Optimizer with weight decay\n",
    "        self.optimizer = optim.AdamW(\n",
    "            self.model.parameters(), \n",
    "            lr=Config.LEARNING_RATE,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        \n",
    "        # Loss function with class weights if imbalanced\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Learning rate scheduler: try verbose=True, fall back if not supported\n",
    "        try:\n",
    "            # Newer PyTorch supports verbose\n",
    "            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "            )\n",
    "            self._scheduler_supports_verbose = True\n",
    "        except TypeError:\n",
    "            # Older PyTorch -> create without verbose\n",
    "            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer, mode='min', factor=0.5, patience=5\n",
    "            )\n",
    "            self._scheduler_supports_verbose = False\n",
    "        \n",
    "        # Keep track of last LR so we can print a message when LR changes (for older torch)\n",
    "        self._last_lr = float(self.optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        # Early stopping\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.patience_counter = 0\n",
    "        \n",
    "        # History\n",
    "        self.history = {\n",
    "            'train_loss': [], 'val_loss': [], \n",
    "            'train_acc': [], 'val_acc': [],\n",
    "            'learning_rates': []\n",
    "        }\n",
    "\n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels, _ in self.train_loader:\n",
    "            images, labels = images.to(self.device), labels.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(self.train_loader.dataset) if len(self.train_loader.dataset)>0 else 0.0\n",
    "        epoch_acc = correct / total if total > 0 else 0.0\n",
    "        \n",
    "        return epoch_loss, epoch_acc\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"Validate model\"\"\"\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels, _ in self.val_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_loss = running_loss / len(self.val_loader.dataset) if len(self.val_loader.dataset)>0 else float('inf')\n",
    "        val_acc = correct / total if total>0 else 0.0\n",
    "        \n",
    "        return val_loss, val_acc, all_preds, all_labels\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        \"\"\"Full training loop\"\"\"\n",
    "        print(f\"\\nTraining on {self.device}\")\n",
    "        print(f\"Train size: {len(self.train_loader.dataset)}\")\n",
    "        print(f\"Val size: {len(self.val_loader.dataset)}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss, train_acc = self.train_epoch()\n",
    "            val_loss, val_acc, _, _ = self.validate()\n",
    "            \n",
    "            # Update scheduler with validation loss\n",
    "            # ReduceLROnPlateau expects the \"metric\" (val_loss) argument\n",
    "            self.scheduler.step(val_loss)\n",
    "            current_lr = float(self.optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "            # If verbose wasn't supported, print a message when LR actually changes\n",
    "            if not self._scheduler_supports_verbose and current_lr < self._last_lr:\n",
    "                print(f\"[LR Scheduler] Reduced LR: {self._last_lr:.6f} -> {current_lr:.6f}\")\n",
    "            self._last_lr = current_lr\n",
    "            \n",
    "            # Save history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_acc'].append(val_acc)\n",
    "            self.history['learning_rates'].append(current_lr)\n",
    "            \n",
    "            print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "            print(f'  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}')\n",
    "            print(f'  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n",
    "            print(f'  LR: {current_lr:.6f}')\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.patience_counter = 0\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_acc': val_acc,\n",
    "                }, Config.MODEL_SAVE_PATH)\n",
    "                print(f'  ✓ Model saved (Val Loss improved)')\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                if self.patience_counter >= Config.EARLY_STOPPING_PATIENCE:\n",
    "                    print(f'\\nEarly stopping triggered after {epoch+1} epochs')\n",
    "                    break\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        return self.history\n",
    "\n",
    "# =============================================================================\n",
    "# UNSUPERVISED LEARNING\n",
    "# =============================================================================\n",
    "\n",
    "class UnsupervisedAnalyzer:\n",
    "    \"\"\"Unsupervised learning for pattern discovery\"\"\"\n",
    "    \n",
    "    def __init__(self, image_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.feature_extractor = PressureMapFeatureExtractor()\n",
    "        self.features_df = None\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def extract_features_from_dataset(self):\n",
    "        \"\"\"Extract features from all images\"\"\"\n",
    "        image_files = [f for f in os.listdir(self.image_dir) \n",
    "                      if f.lower().endswith(('.jpg', '.jpeg', '.png', '.tiff', '.bmp'))]\n",
    "        \n",
    "        features_list = []\n",
    "        valid_files = []\n",
    "        \n",
    "        print(f\"Extracting features from {len(image_files)} images...\")\n",
    "        for img_file in image_files:\n",
    "            img_path = os.path.join(self.image_dir, img_file)\n",
    "            features = self.feature_extractor.extract_all_features(img_path)\n",
    "            if features is not None:\n",
    "                features['filename'] = img_file\n",
    "                features_list.append(features)\n",
    "                valid_files.append(img_file)\n",
    "        \n",
    "        self.features_df = pd.DataFrame(features_list)\n",
    "        print(f\"Extracted features from {len(features_list)} images\")\n",
    "        \n",
    "        # Save features\n",
    "        features_path = os.path.join(Config.FEATURES_DIR, 'extracted_features.csv')\n",
    "        try:\n",
    "            self.features_df.to_csv(features_path, index=False, encoding='utf-8-sig')\n",
    "        except Exception:\n",
    "            self.features_df.to_csv(features_path, index=False)\n",
    "        print(f\"Features saved to {features_path}\")\n",
    "        \n",
    "        return self.features_df\n",
    "    \n",
    "    def perform_clustering(self, n_clusters=3, method='kmeans'):\n",
    "        \"\"\"Perform clustering analysis\"\"\"\n",
    "        if self.features_df is None:\n",
    "            print(\"No features available. Extract features first.\")\n",
    "            return None\n",
    "        \n",
    "        # Prepare features (exclude filename)\n",
    "        feature_cols = [col for col in self.features_df.columns if col != 'filename']\n",
    "        X = self.features_df[feature_cols].values\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Clustering\n",
    "        if method == 'kmeans':\n",
    "            clusterer = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        elif method == 'dbscan':\n",
    "            clusterer = DBSCAN(eps=0.5, min_samples=5)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "        \n",
    "        labels = clusterer.fit_predict(X_scaled)\n",
    "        \n",
    "        # Add labels to dataframe\n",
    "        self.features_df['cluster'] = labels\n",
    "        \n",
    "        # Calculate silhouette score if more than 1 cluster\n",
    "        if len(set(labels)) > 1:\n",
    "            try:\n",
    "                sil_score = silhouette_score(X_scaled, labels)\n",
    "                print(f\"Silhouette Score: {sil_score:.3f}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        # Save clustering results\n",
    "        results_path = os.path.join(Config.RESULTS_DIR, f'clustering_{method}_results.csv')\n",
    "        try:\n",
    "            self.features_df.to_csv(results_path, index=False, encoding='utf-8-sig')\n",
    "        except Exception:\n",
    "            self.features_df.to_csv(results_path, index=False)\n",
    "        print(f\"Clustering results saved to {results_path}\")\n",
    "        \n",
    "        return labels, X_scaled\n",
    "    \n",
    "    def visualize_clusters(self, X_scaled, labels):\n",
    "        \"\"\"Visualize clustering results using PCA\"\"\"\n",
    "        # PCA for visualization\n",
    "        pca = PCA(n_components=2)\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Scatter plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, \n",
    "                            cmap='viridis', alpha=0.6, edgecolors='k')\n",
    "        plt.colorbar(scatter, label='Cluster')\n",
    "        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "        plt.title('Cluster Visualization (PCA)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Cluster distribution\n",
    "        plt.subplot(1, 2, 2)\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        plt.bar(unique, counts, color='skyblue', edgecolor='black')\n",
    "        plt.xlabel('Cluster')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Cluster Distribution')\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_path = os.path.join(Config.RESULTS_DIR, 'clustering_visualization.png')\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Visualization saved to {plot_path}\")\n",
    "        \n",
    "        return X_pca\n",
    "\n",
    "# =============================================================================\n",
    "# INTERPRETABILITY & VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "class GradCAM:\n",
    "    \"\"\"Gradient-weighted Class Activation Mapping for interpretability\"\"\"\n",
    "    \n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        # Register hooks\n",
    "        target_layer.register_forward_hook(self.save_activation)\n",
    "        target_layer.register_backward_hook(self.save_gradient)\n",
    "    \n",
    "    def save_activation(self, module, input, output):\n",
    "        self.activations = output.detach()\n",
    "    \n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0].detach()\n",
    "    \n",
    "    def generate_cam(self, input_image, target_class=None):\n",
    "        \"\"\"Generate Class Activation Map\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = self.model(input_image)\n",
    "        \n",
    "        if target_class is None:\n",
    "            target_class = output.argmax(dim=1).item()\n",
    "        \n",
    "        # Backward pass\n",
    "        self.model.zero_grad()\n",
    "        target = output[0, target_class]\n",
    "        target.backward()\n",
    "        \n",
    "        # Get weights\n",
    "        weights = self.gradients.mean(dim=(2, 3), keepdim=True)\n",
    "        \n",
    "        # Weighted combination\n",
    "        cam = (weights * self.activations).sum(dim=1, keepdim=True)\n",
    "        cam = torch.relu(cam)\n",
    "        \n",
    "        # Normalize\n",
    "        cam = cam - cam.min()\n",
    "        cam = cam / (cam.max() + 1e-8)\n",
    "        \n",
    "        return cam.squeeze().cpu().numpy(), target_class\n",
    "\n",
    "# =============================================================================\n",
    "# INTERACTIVE LABELING TOOL (CORRECTED)\n",
    "# =============================================================================\n",
    "\n",
    "class LabelingTool:\n",
    "    \"\"\"Interactive tool for manual labeling (robust image display & resizing)\"\"\"\n",
    "    \n",
    "    def __init__(self, root, image_dir):\n",
    "        self.root = root\n",
    "        self.image_dir = image_dir\n",
    "        self.current_idx = 0\n",
    "        self.labels = {}\n",
    "        self.current_image = None   # PIL Image of currently loaded image (original)\n",
    "        self.current_photo = None   # ImageTk.PhotoImage reference\n",
    "        \n",
    "        # Get image files\n",
    "        self.image_files = [f for f in os.listdir(image_dir)\n",
    "                          if f.lower().endswith(('.jpg', '.jpeg', '.png', '.tiff', '.bmp'))]\n",
    "        self.image_files.sort()\n",
    "        \n",
    "        # Load existing labels if any\n",
    "        if os.path.exists(Config.LABELS_CSV):\n",
    "            try:\n",
    "                df = pd.read_csv(Config.LABELS_CSV)\n",
    "                self.labels = dict(zip(df['image_path'], df['label']))\n",
    "                print(f\"Loaded {len(self.labels)} existing labels\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        self.create_widgets()\n",
    "        # Wait until the window is visible to load the first image\n",
    "        self.root.after(100, lambda: self.load_image(self.current_idx))\n",
    "    \n",
    "    def create_widgets(self):\n",
    "        \"\"\"Create labeling interface\"\"\"\n",
    "        self.root.title(\"Plantar Pressure Image Labeling Tool\")\n",
    "        self.root.geometry(\"900x700\")\n",
    "        \n",
    "        # Top info bar\n",
    "        info_frame = ttk.Frame(self.root)\n",
    "        info_frame.pack(fill=tk.X, padx=10, pady=5)\n",
    "        \n",
    "        self.progress_label = ttk.Label(info_frame, \n",
    "                                       text=f\"Image 0/{len(self.image_files)}\", \n",
    "                                       font=('Arial', 12, 'bold'))\n",
    "        self.progress_label.pack(side=tk.LEFT)\n",
    "        \n",
    "        labeled_count = len([v for v in self.labels.values() if v])\n",
    "        self.labeled_label = ttk.Label(info_frame, \n",
    "                                       text=f\"Labeled: {labeled_count}\", \n",
    "                                       font=('Arial', 12))\n",
    "        self.labeled_label.pack(side=tk.RIGHT)\n",
    "        \n",
    "        # Image display\n",
    "        img_frame = ttk.LabelFrame(self.root, text=\"Plantar Pressure Map\")\n",
    "        img_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)\n",
    "        \n",
    "        # Create canvas with a default size; allow it to expand\n",
    "        self.canvas = tk.Canvas(img_frame, bg='black', highlightthickness=1, highlightbackground='#cccccc')\n",
    "        self.canvas.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        # Bind resize event to redraw correctly\n",
    "        self.canvas.bind(\"<Configure>\", self._on_canvas_configure)\n",
    "        \n",
    "        # Filename display\n",
    "        self.filename_label = ttk.Label(self.root, text=\"\", \n",
    "                                       font=('Arial', 10))\n",
    "        self.filename_label.pack(pady=2)\n",
    "        \n",
    "        # Labeling buttons\n",
    "        button_frame = ttk.LabelFrame(self.root, text=\"Classification\")\n",
    "        button_frame.pack(fill=tk.X, padx=10, pady=5)\n",
    "        \n",
    "        # ttk.Button doesn't accept 'height' param in same way as tk.Button; avoid non-portable options\n",
    "        btn_frame_inner = ttk.Frame(button_frame)\n",
    "        btn_frame_inner.pack(pady=10)\n",
    "        \n",
    "        ttk.Button(btn_frame_inner, text=\"Normal\", command=lambda: self.label_current('healthy')).pack(side=tk.LEFT, padx=10)\n",
    "        ttk.Button(btn_frame_inner, text=\"Abnormal\", command=lambda: self.label_current('unhealthy')).pack(side=tk.LEFT, padx=10)\n",
    "        ttk.Button(btn_frame_inner, text=\"Skip\", command=self.skip_image).pack(side=tk.LEFT, padx=10)\n",
    "        \n",
    "        # Navigation\n",
    "        nav_frame = ttk.Frame(self.root)\n",
    "        nav_frame.pack(fill=tk.X, padx=10, pady=5)\n",
    "        \n",
    "        ttk.Button(nav_frame, text=\"◄ Previous\", command=self.prev_image).pack(side=tk.LEFT, padx=5)\n",
    "        ttk.Button(nav_frame, text=\"Next ►\", command=self.next_image).pack(side=tk.LEFT, padx=5)\n",
    "        ttk.Button(nav_frame, text=\"Save & Exit\", command=self.save_and_exit).pack(side=tk.RIGHT, padx=5)\n",
    "        \n",
    "        # Keyboard shortcuts\n",
    "        self.root.bind('1', lambda e: self.label_current('healthy'))\n",
    "        self.root.bind('2', lambda e: self.label_current('unhealthy'))\n",
    "        self.root.bind('<space>', lambda e: self.skip_image())\n",
    "        self.root.bind('<Left>', lambda e: self.prev_image())\n",
    "        self.root.bind('<Right>', lambda e: self.next_image())\n",
    "    \n",
    "    def _on_canvas_configure(self, event):\n",
    "        \"\"\"Redraw current image when the canvas size changes\"\"\"\n",
    "        # If we already have an image loaded, rescale and redraw it\n",
    "        if self.current_image is not None:\n",
    "            self._display_image_on_canvas(self.current_image)\n",
    "    \n",
    "    def load_image(self, idx):\n",
    "        \"\"\"Load and display image at index idx\"\"\"\n",
    "        if idx < 0 or idx >= len(self.image_files):\n",
    "            return\n",
    "        \n",
    "        self.current_idx = idx\n",
    "        filename = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, filename)\n",
    "        \n",
    "        try:\n",
    "            # Load original PIL image and keep it for resizing/redraw\n",
    "            pil_image = Image.open(img_path).convert('RGB')\n",
    "            self.current_image = pil_image  # store original image\n",
    "            \n",
    "            # Immediately attempt to display (display will scale to current canvas size)\n",
    "            self._display_image_on_canvas(pil_image)\n",
    "            \n",
    "            # Update labels\n",
    "            self.progress_label.config(text=f\"Image {idx+1}/{len(self.image_files)}\")\n",
    "            \n",
    "            current_label = self.labels.get(img_path, \"Not labeled\")\n",
    "            self.filename_label.config(text=f\"{filename} | Current: {current_label}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Failed to load image: {e}\")\n",
    "            self.current_image = None\n",
    "            self.canvas.delete(\"all\")\n",
    "    \n",
    "    def _display_image_on_canvas(self, pil_image):\n",
    "        \"\"\"Scale pil_image to the current canvas size and display it\"\"\"\n",
    "        # Compute available drawing area (subtract small padding)\n",
    "        canvas_width = max(1, self.canvas.winfo_width())\n",
    "        canvas_height = max(1, self.canvas.winfo_height())\n",
    "        \n",
    "        # If canvas is not yet realized (very small), schedule re-draw soon\n",
    "        if canvas_width < 10 or canvas_height < 10:\n",
    "            # Try again shortly after mainloop is idle\n",
    "            self.root.after(100, lambda: self._display_image_on_canvas(pil_image))\n",
    "            return\n",
    "        \n",
    "        # Determine max allowed size (leave small margins)\n",
    "        max_w = max(1, canvas_width - 10)\n",
    "        max_h = max(1, canvas_height - 10)\n",
    "        \n",
    "        # Create a copy to avoid modifying original\n",
    "        image_copy = pil_image.copy()\n",
    "        image_copy.thumbnail((max_w, max_h), RESAMPLE_LANCZOS)\n",
    "        \n",
    "        # Create PhotoImage and keep a reference so it is not garbage-collected\n",
    "        self.current_photo = ImageTk.PhotoImage(image_copy)\n",
    "        \n",
    "        # Clear canvas and draw image anchored at top-left (north-west)\n",
    "        self.canvas.delete(\"all\")\n",
    "        self.canvas.create_image(0, 0, image=self.current_photo, anchor='nw')\n",
    "    \n",
    "    def label_current(self, label):\n",
    "        \"\"\"Label current image\"\"\"\n",
    "        filename = self.image_files[self.current_idx]\n",
    "        img_path = os.path.join(self.image_dir, filename)\n",
    "        self.labels[img_path] = label\n",
    "        \n",
    "        # Update labeled count\n",
    "        labeled_count = len([v for v in self.labels.values() if v])\n",
    "        self.labeled_label.config(text=f\"Labeled: {labeled_count}\")\n",
    "        \n",
    "        # Move to next\n",
    "        self.next_image()\n",
    "    \n",
    "    def skip_image(self):\n",
    "        \"\"\"Skip current image\"\"\"\n",
    "        self.next_image()\n",
    "    \n",
    "    def next_image(self):\n",
    "        \"\"\"Go to next image\"\"\"\n",
    "        if self.current_idx < len(self.image_files) - 1:\n",
    "            self.load_image(self.current_idx + 1)\n",
    "    \n",
    "    def prev_image(self):\n",
    "        \"\"\"Go to previous image\"\"\"\n",
    "        if self.current_idx > 0:\n",
    "            self.load_image(self.current_idx - 1)\n",
    "    \n",
    "    def save_and_exit(self):\n",
    "        \"\"\"Save labels and exit\"\"\"\n",
    "        if not self.labels:\n",
    "            messagebox.showwarning(\"Warning\", \"No labels to save!\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrame\n",
    "        data = []\n",
    "        for img_path, label in self.labels.items():\n",
    "            data.append({'image_path': img_path, 'label': label})\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        try:\n",
    "            df.to_csv(Config.LABELS_CSV, index=False, encoding='utf-8-sig')\n",
    "        except Exception:\n",
    "            df.to_csv(Config.LABELS_CSV, index=False)\n",
    "        \n",
    "        messagebox.showinfo(\"Success\", f\"Saved {len(self.labels)} labels to {Config.LABELS_CSV}\")\n",
    "        self.root.destroy()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN APPLICATION\n",
    "# =============================================================================\n",
    "\n",
    "class ProfessionalPlantarApp:\n",
    "    \"\"\"Main application with all features\"\"\"\n",
    "    \n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Professional Plantar Pressure Analysis System\")\n",
    "        self.root.geometry(\"1200x800\")\n",
    "        \n",
    "        Config.create_directories()\n",
    "        \n",
    "        # Initialize components\n",
    "        self.model = None\n",
    "        self.unsupervised_analyzer = UnsupervisedAnalyzer(Config.DATASET_DIR)\n",
    "        self.feature_extractor = PressureMapFeatureExtractor()\n",
    "        \n",
    "        # Transform for prediction\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(Config.IMAGE_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.create_gui()\n",
    "        self.load_model_if_exists()\n",
    "    \n",
    "    def create_gui(self):\n",
    "        \"\"\"Create main interface\"\"\"\n",
    "        # Menu bar\n",
    "        menubar = tk.Menu(self.root)\n",
    "        self.root.config(menu=menubar)\n",
    "        \n",
    "        # File menu\n",
    "        file_menu = tk.Menu(menubar, tearoff=0)\n",
    "        menubar.add_cascade(label=\"File\", menu=file_menu)\n",
    "        file_menu.add_command(label=\"Label Images\", command=self.open_labeling_tool)\n",
    "        file_menu.add_separator()\n",
    "        file_menu.add_command(label=\"Exit\", command=self.root.quit)\n",
    "        \n",
    "        # Analysis menu\n",
    "        analysis_menu = tk.Menu(menubar, tearoff=0)\n",
    "        menubar.add_cascade(label=\"Analysis\", menu=analysis_menu)\n",
    "        analysis_menu.add_command(label=\"Extract Features\", \n",
    "                                 command=self.extract_features)\n",
    "        analysis_menu.add_command(label=\"Cluster Analysis\", \n",
    "                                 command=self.run_clustering)\n",
    "        analysis_menu.add_command(label=\"View Results\", \n",
    "                                 command=self.view_results)\n",
    "        \n",
    "        # Model menu\n",
    "        model_menu = tk.Menu(menubar, tearoff=0)\n",
    "        menubar.add_cascade(label=\"Model\", menu=model_menu)\n",
    "        model_menu.add_command(label=\"Train Model\", command=self.train_model)\n",
    "        model_menu.add_command(label=\"Load Model\", command=self.load_model_if_exists)\n",
    "        \n",
    "        # Main notebook\n",
    "        self.notebook = ttk.Notebook(self.root)\n",
    "        self.notebook.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "        \n",
    "        # Prediction tab\n",
    "        self.create_prediction_tab()\n",
    "        \n",
    "        # Analysis tab\n",
    "        self.create_analysis_tab()\n",
    "        \n",
    "        # Status bar\n",
    "        self.status_var = tk.StringVar(value=\"Ready\")\n",
    "        status_bar = ttk.Label(self.root, textvariable=self.status_var, \n",
    "                              relief=tk.SUNKEN, anchor=tk.W)\n",
    "        status_bar.pack(side=tk.BOTTOM, fill=tk.X)\n",
    "    \n",
    "    def create_prediction_tab(self):\n",
    "        \"\"\"Create prediction interface\"\"\"\n",
    "        pred_frame = ttk.Frame(self.notebook)\n",
    "        self.notebook.add(pred_frame, text=\"Image Prediction\")\n",
    "        \n",
    "        # Control panel\n",
    "        control_frame = ttk.LabelFrame(pred_frame, text=\"Controls\")\n",
    "        control_frame.pack(fill=tk.X, padx=10, pady=5)\n",
    "        \n",
    "        ttk.Button(control_frame, text=\"Load Image\", \n",
    "                  command=self.load_and_predict).pack(side=tk.LEFT, padx=5, pady=5)\n",
    "        ttk.Button(control_frame, text=\"Batch Predict\", \n",
    "                  command=self.batch_predict).pack(side=tk.LEFT, padx=5, pady=5)\n",
    "        \n",
    "        # Content area (split)\n",
    "        content_frame = ttk.Frame(pred_frame)\n",
    "        content_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)\n",
    "        \n",
    "        # Left: Image display\n",
    "        left_frame = ttk.LabelFrame(content_frame, text=\"Image\")\n",
    "        left_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=5)\n",
    "        \n",
    "        self.image_canvas = tk.Canvas(left_frame, bg='black')\n",
    "        self.image_canvas.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        self.image_canvas.bind(\"<Configure>\", lambda e: None)  # keep to allow future resizing behavior\n",
    "        \n",
    "        # Right: Results\n",
    "        right_frame = ttk.LabelFrame(content_frame, text=\"Results\")\n",
    "        right_frame.pack(side=tk.RIGHT, fill=tk.BOTH, expand=True, padx=5)\n",
    "        \n",
    "        # Prediction result\n",
    "        result_frame = ttk.Frame(right_frame)\n",
    "        result_frame.pack(fill=tk.X, pady=10, padx=10)\n",
    "        \n",
    "        ttk.Label(result_frame, text=\"Classification:\", \n",
    "                 font=('Arial', 11, 'bold')).pack(anchor=tk.W)\n",
    "        self.pred_result_var = tk.StringVar(value=\"No prediction\")\n",
    "        self.pred_result_label = ttk.Label(result_frame, \n",
    "                                          textvariable=self.pred_result_var,\n",
    "                                          font=('Arial', 14, 'bold'))\n",
    "        self.pred_result_label.pack(anchor=tk.W, pady=5)\n",
    "        \n",
    "        # Confidence\n",
    "        conf_frame = ttk.LabelFrame(right_frame, text=\"Confidence\")\n",
    "        conf_frame.pack(fill=tk.X, pady=10, padx=10)\n",
    "        \n",
    "        self.conf_text = tk.Text(conf_frame, height=3, font=('Consolas', 10))\n",
    "        self.conf_text.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        # Features\n",
    "        feat_frame = ttk.LabelFrame(right_frame, text=\"Extracted Features\")\n",
    "        feat_frame.pack(fill=tk.BOTH, expand=True, pady=10, padx=10)\n",
    "        \n",
    "        self.feat_text = tk.Text(feat_frame, font=('Consolas', 9))\n",
    "        feat_scrollbar = ttk.Scrollbar(feat_frame, command=self.feat_text.yview)\n",
    "        self.feat_text.configure(yscrollcommand=feat_scrollbar.set)\n",
    "        feat_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "        self.feat_text.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "    \n",
    "    def create_analysis_tab(self):\n",
    "        \"\"\"Create analysis interface\"\"\"\n",
    "        analysis_frame = ttk.Frame(self.notebook)\n",
    "        self.notebook.add(analysis_frame, text=\"Dataset Analysis\")\n",
    "        \n",
    "        # Analysis text area\n",
    "        self.analysis_text = tk.Text(analysis_frame, font=('Consolas', 10))\n",
    "        scrollbar = ttk.Scrollbar(analysis_frame, command=self.analysis_text.yview)\n",
    "        self.analysis_text.configure(yscrollcommand=scrollbar.set)\n",
    "        \n",
    "        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "        self.analysis_text.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "    \n",
    "    def open_labeling_tool(self):\n",
    "        \"\"\"Open labeling tool in new window\"\"\"\n",
    "        labeling_window = tk.Toplevel(self.root)\n",
    "        LabelingTool(labeling_window, Config.DATASET_DIR)\n",
    "    \n",
    "    def extract_features(self):\n",
    "        \"\"\"Extract features from all images\"\"\"\n",
    "        self.status_var.set(\"Extracting features...\")\n",
    "        self.root.update()\n",
    "        \n",
    "        def extract_thread():\n",
    "            try:\n",
    "                features_df = self.unsupervised_analyzer.extract_features_from_dataset()\n",
    "                \n",
    "                # Display summary\n",
    "                summary = f\"Feature Extraction Complete\\n\"\n",
    "                summary += f\"{'='*50}\\n\\n\"\n",
    "                summary += f\"Total images processed: {len(features_df)}\\n\"\n",
    "                summary += f\"Total features: {len(features_df.columns)-1}\\n\\n\"\n",
    "                summary += f\"Feature Statistics:\\n\"\n",
    "                summary += f\"{'-'*50}\\n\"\n",
    "                summary += features_df.describe().to_string()\n",
    "                \n",
    "                self.analysis_text.delete('1.0', tk.END)\n",
    "                self.analysis_text.insert('1.0', summary)\n",
    "                self.status_var.set(\"Feature extraction complete\")\n",
    "            except Exception as e:\n",
    "                self.status_var.set(f\"Error: {e}\")\n",
    "                messagebox.showerror(\"Error\", str(e))\n",
    "        \n",
    "        threading.Thread(target=extract_thread, daemon=True).start()\n",
    "    \n",
    "    def run_clustering(self):\n",
    "        \"\"\"Run clustering analysis\"\"\"\n",
    "        self.status_var.set(\"Running clustering...\")\n",
    "        self.root.update()\n",
    "        \n",
    "        def cluster_thread():\n",
    "            try:\n",
    "                labels, X_scaled = self.unsupervised_analyzer.perform_clustering(\n",
    "                    n_clusters=Config.N_CLUSTERS\n",
    "                )\n",
    "                \n",
    "                X_pca = self.unsupervised_analyzer.visualize_clusters(X_scaled, labels)\n",
    "                \n",
    "                # Display results\n",
    "                summary = f\"Clustering Analysis Complete\\n\"\n",
    "                summary += f\"{'='*50}\\n\\n\"\n",
    "                summary += f\"Method: K-Means\\n\"\n",
    "                summary += f\"Number of clusters: {Config.N_CLUSTERS}\\n\\n\"\n",
    "                summary += f\"Cluster Distribution:\\n\"\n",
    "                summary += f\"{'-'*50}\\n\"\n",
    "                \n",
    "                for cluster_id in range(Config.N_CLUSTERS):\n",
    "                    count = np.sum(labels == cluster_id)\n",
    "                    percentage = (count / len(labels)) * 100\n",
    "                    summary += f\"Cluster {cluster_id}: {count} images ({percentage:.1f}%)\\n\"\n",
    "                \n",
    "                summary += f\"\\n\\nInterpretation:\\n\"\n",
    "                summary += f\"{'-'*50}\\n\"\n",
    "                summary += f\"Images have been grouped into {Config.N_CLUSTERS} distinct patterns.\\n\"\n",
    "                summary += f\"Review 'clustering_kmeans_results.csv' to see which images\\n\"\n",
    "                summary += f\"belong to each cluster. This can help identify:\\n\"\n",
    "                summary += f\"  - Normal vs abnormal patterns\\n\"\n",
    "                summary += f\"  - Different types of gait abnormalities\\n\"\n",
    "                summary += f\"  - Outliers or unusual cases\\n\"\n",
    "                \n",
    "                self.analysis_text.delete('1.0', tk.END)\n",
    "                self.analysis_text.insert('1.0', summary)\n",
    "                self.status_var.set(\"Clustering complete - check results folder\")\n",
    "                \n",
    "                messagebox.showinfo(\"Success\", \n",
    "                                  \"Clustering complete! Check 'results' folder for visualizations.\")\n",
    "            except Exception as e:\n",
    "                self.status_var.set(f\"Error: {e}\")\n",
    "                messagebox.showerror(\"Error\", str(e))\n",
    "        \n",
    "        threading.Thread(target=cluster_thread, daemon=True).start()\n",
    "    \n",
    "    def view_results(self):\n",
    "        \"\"\"View analysis results\"\"\"\n",
    "        results_dir = Config.RESULTS_DIR\n",
    "        if os.path.exists(results_dir):\n",
    "            try:\n",
    "                os.startfile(results_dir)  # Windows\n",
    "            except Exception:\n",
    "                messagebox.showinfo(\"Info\", f\"Results path: {os.path.abspath(results_dir)}\")\n",
    "        else:\n",
    "            messagebox.showinfo(\"Info\", \"No results available yet\")\n",
    "    \n",
    "    def load_model_if_exists(self):\n",
    "        \"\"\"Load trained model if available\"\"\"\n",
    "        if os.path.exists(Config.MODEL_SAVE_PATH):\n",
    "            try:\n",
    "                self.model = ImprovedPlantarCNN().to(Config.DEVICE)\n",
    "                checkpoint = torch.load(Config.MODEL_SAVE_PATH, map_location=Config.DEVICE)\n",
    "                self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                self.model.eval()\n",
    "                self.status_var.set(f\"Model loaded successfully (Val Acc: {checkpoint.get('val_acc', 0):.2%})\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                self.status_var.set(f\"Error loading model: {e}\")\n",
    "                return False\n",
    "        else:\n",
    "            self.status_var.set(\"No trained model found\")\n",
    "            return False\n",
    "    \n",
    "    def train_model(self):\n",
    "        \"\"\"Train supervised model\"\"\"\n",
    "        if not os.path.exists(Config.LABELS_CSV):\n",
    "            messagebox.showerror(\"Error\", \n",
    "                               \"No labels file found! Please label images first.\")\n",
    "            return\n",
    "        \n",
    "        response = messagebox.askyesno(\"Train Model\", \n",
    "                                      \"This will start training. Continue?\")\n",
    "        if not response:\n",
    "            return\n",
    "        \n",
    "        self.status_var.set(\"Training started...\")\n",
    "        \n",
    "        def train_thread():\n",
    "            try:\n",
    "                # Data augmentation\n",
    "                train_transform = transforms.Compose([\n",
    "                    transforms.Resize(Config.IMAGE_SIZE),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomRotation(15),\n",
    "                    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                       std=[0.229, 0.224, 0.225])\n",
    "                ])\n",
    "                \n",
    "                val_transform = transforms.Compose([\n",
    "                    transforms.Resize(Config.IMAGE_SIZE),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                       std=[0.229, 0.224, 0.225])\n",
    "                ])\n",
    "                \n",
    "                # Load dataset\n",
    "                full_dataset = PlantarDataset(Config.LABELS_CSV, transform=train_transform)\n",
    "                \n",
    "                # Split (handle small datasets)\n",
    "                if len(full_dataset) < 2:\n",
    "                    messagebox.showerror(\"Error\", \"Not enough labeled examples to train.\")\n",
    "                    self.status_var.set(\"Training aborted: insufficient data\")\n",
    "                    return\n",
    "                \n",
    "                train_size = int(0.8 * len(full_dataset))\n",
    "                val_size = len(full_dataset) - train_size\n",
    "                train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "                \n",
    "                # Loaders\n",
    "                train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, \n",
    "                                        shuffle=True, num_workers=2)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE, \n",
    "                                      num_workers=2)\n",
    "                \n",
    "                # Model\n",
    "                model = ImprovedPlantarCNN(pretrained=True)\n",
    "                \n",
    "                # Training\n",
    "                engine = TrainingEngine(model, train_loader, val_loader, Config.DEVICE)\n",
    "                history = engine.train(Config.EPOCHS)\n",
    "                \n",
    "                self.status_var.set(\"Training complete!\")\n",
    "                messagebox.showinfo(\"Success\", \"Model training complete!\")\n",
    "                \n",
    "                # Load the trained model\n",
    "                self.load_model_if_exists()\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.status_var.set(f\"Training error: {e}\")\n",
    "                messagebox.showerror(\"Error\", str(e))\n",
    "        \n",
    "        threading.Thread(target=train_thread, daemon=True).start()\n",
    "    \n",
    "    def load_and_predict(self):\n",
    "        \"\"\"Load image and make prediction\"\"\"\n",
    "        if self.model is None:\n",
    "            messagebox.showerror(\"Error\", \"No model loaded!\")\n",
    "            return\n",
    "        \n",
    "        file_path = filedialog.askopenfilename(\n",
    "            filetypes=[(\"Image files\", \"*.jpg;*.jpeg;*.png;*.tiff;*.bmp\")]\n",
    "        )\n",
    "        if not file_path:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Load and display - robust resizing similar to labeling tool\n",
    "            image = Image.open(file_path).convert('RGB')\n",
    "            display_img = image.copy()\n",
    "            # scale to canvas size\n",
    "            canvas_width = max(1, self.image_canvas.winfo_width())\n",
    "            canvas_height = max(1, self.image_canvas.winfo_height())\n",
    "            if canvas_width > 10 and canvas_height > 10:\n",
    "                display_img.thumbnail((canvas_width-10, canvas_height-10), RESAMPLE_LANCZOS)\n",
    "            \n",
    "            photo = ImageTk.PhotoImage(display_img)\n",
    "            \n",
    "            self.image_canvas.delete(\"all\")\n",
    "            # Keep reference\n",
    "            self.image_canvas.image = photo\n",
    "            # Draw top-left\n",
    "            self.image_canvas.create_image(0, 0, image=photo, anchor='nw')\n",
    "            \n",
    "            # Predict\n",
    "            input_tensor = self.transform(image).unsqueeze(0).to(Config.DEVICE)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_tensor)\n",
    "                probabilities = torch.softmax(outputs, dim=1)[0]\n",
    "            \n",
    "            # Get results\n",
    "            classes = ['Normal (Healthy)', 'Abnormal (Unhealthy)']\n",
    "            pred_class = int(probabilities.argmax().item())\n",
    "            confidence = float(probabilities[pred_class].item() * 100)\n",
    "            \n",
    "            # Update UI\n",
    "            self.pred_result_var.set(classes[pred_class])\n",
    "            color = \"green\" if pred_class == 0 else \"red\"\n",
    "            self.pred_result_label.config(foreground=color)\n",
    "            \n",
    "            # Confidence details\n",
    "            conf_text = f\"Normal:   {probabilities[0].item()*100:.2f}%\\n\"\n",
    "            conf_text += f\"Abnormal: {probabilities[1].item()*100:.2f}%\\n\"\n",
    "            conf_text += f\"Confidence: {confidence:.2f}%\"\n",
    "            \n",
    "            self.conf_text.delete('1.0', tk.END)\n",
    "            self.conf_text.insert('1.0', conf_text)\n",
    "            \n",
    "            # Extract and display features\n",
    "            features = self.feature_extractor.extract_all_features(file_path)\n",
    "            if features:\n",
    "                feat_text = \"Key Features:\\n\" + \"=\"*40 + \"\\n\\n\"\n",
    "                for key, value in sorted(features.items()):\n",
    "                    if key != 'filename':\n",
    "                        try:\n",
    "                            feat_text += f\"{key:30s}: {float(value):.4f}\\n\"\n",
    "                        except Exception:\n",
    "                            feat_text += f\"{key:30s}: {value}\\n\"\n",
    "                \n",
    "                self.feat_text.delete('1.0', tk.END)\n",
    "                self.feat_text.insert('1.0', feat_text)\n",
    "            \n",
    "            self.status_var.set(f\"Prediction complete: {classes[pred_class]}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.status_var.set(f\"Error: {e}\")\n",
    "            messagebox.showerror(\"Error\", str(e))\n",
    "    \n",
    "    def batch_predict(self):\n",
    "        \"\"\"Batch prediction on folder\"\"\"\n",
    "        if self.model is None:\n",
    "            messagebox.showerror(\"Error\", \"No model loaded!\")\n",
    "            return\n",
    "        \n",
    "        folder_path = filedialog.askdirectory(title=\"Select folder with images\")\n",
    "        if not folder_path:\n",
    "            return\n",
    "        \n",
    "        # Implementation for batch prediction\n",
    "        messagebox.showinfo(\"Info\", \"Batch prediction feature - coming soon!\")\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN ENTRY POINT\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*60)\n",
    "    print(\"Professional Plantar Pressure Analysis System\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Device: {Config.DEVICE}\")\n",
    "    print(f\"Dataset: {Config.DATASET_DIR}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    Config.create_directories()\n",
    "    \n",
    "    root = tk.Tk()\n",
    "    app = ProfessionalPlantarApp(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bfbf59-56d3-480a-8aa9-781d82a244a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Professional Plantar Pressure Analysis System\n",
      "============================================================\n",
      "Device: cuda\n",
      "Dataset: C:\\Users\\koust\\AnaKonda\\FOOT_PLANTAR_CLASSIFICATION\\Dataset\n",
      "============================================================\n",
      "Loaded 202 existing labels\n",
      "Extracting features from 202 images...\n",
      "Extracted features from 202 images\n",
      "Features saved to features\\extracted_features.csv\n",
      "Silhouette Score: 0.160\n",
      "Clustering results saved to results\\clustering_kmeans_results.csv\n",
      "Visualization saved to results\\clustering_visualization.png\n",
      "\n",
      "Training on cuda\n",
      "Train size: 161\n",
      "Val size: 41\n",
      "============================================================\n",
      "Epoch 1/100\n",
      "  Train Loss: 0.7161 | Train Acc: 0.4596\n",
      "  Val Loss: 0.7031 | Val Acc: 0.4634\n",
      "  LR: 0.000100\n",
      "  ✓ Model saved (Val Loss improved)\n",
      "\n",
      "Epoch 2/100\n",
      "  Train Loss: 0.6995 | Train Acc: 0.5404\n",
      "  Val Loss: 0.7065 | Val Acc: 0.4390\n",
      "  LR: 0.000100\n",
      "\n",
      "Epoch 3/100\n",
      "  Train Loss: 0.6774 | Train Acc: 0.5404\n",
      "  Val Loss: 0.6872 | Val Acc: 0.5122\n",
      "  LR: 0.000100\n",
      "  ✓ Model saved (Val Loss improved)\n",
      "\n",
      "Epoch 4/100\n",
      "  Train Loss: 0.6883 | Train Acc: 0.5714\n",
      "  Val Loss: 0.6754 | Val Acc: 0.7073\n",
      "  LR: 0.000100\n",
      "  ✓ Model saved (Val Loss improved)\n",
      "\n",
      "Epoch 5/100\n",
      "  Train Loss: 0.6895 | Train Acc: 0.5466\n",
      "  Val Loss: 0.6770 | Val Acc: 0.6341\n",
      "  LR: 0.000100\n",
      "\n",
      "Epoch 6/100\n",
      "  Train Loss: 0.6827 | Train Acc: 0.4907\n",
      "  Val Loss: 0.7192 | Val Acc: 0.4146\n",
      "  LR: 0.000100\n",
      "\n",
      "Epoch 7/100\n",
      "  Train Loss: 0.6574 | Train Acc: 0.6398\n",
      "  Val Loss: 0.7063 | Val Acc: 0.4146\n",
      "  LR: 0.000100\n",
      "\n",
      "Epoch 8/100\n",
      "  Train Loss: 0.6435 | Train Acc: 0.6770\n",
      "  Val Loss: 0.6932 | Val Acc: 0.4878\n",
      "  LR: 0.000100\n",
      "\n",
      "Epoch 9/100\n",
      "  Train Loss: 0.6409 | Train Acc: 0.6398\n",
      "  Val Loss: 0.7156 | Val Acc: 0.3902\n",
      "  LR: 0.000100\n",
      "\n",
      "Epoch 10/100\n",
      "  Train Loss: 0.6440 | Train Acc: 0.6708\n",
      "  Val Loss: 0.6719 | Val Acc: 0.6585\n",
      "  LR: 0.000100\n",
      "  ✓ Model saved (Val Loss improved)\n",
      "\n",
      "Epoch 11/100\n",
      "  Train Loss: 0.6233 | Train Acc: 0.6273\n",
      "  Val Loss: 0.7248 | Val Acc: 0.6341\n",
      "  LR: 0.000100\n",
      "\n",
      "Epoch 12/100\n",
      "  Train Loss: 0.6084 | Train Acc: 0.6770\n",
      "  Val Loss: 0.7086 | Val Acc: 0.5122\n",
      "  LR: 0.000100\n",
      "\n",
      "Epoch 13/100\n",
      "  Train Loss: 0.6197 | Train Acc: 0.6584\n",
      "  Val Loss: 0.8243 | Val Acc: 0.4634\n",
      "  LR: 0.000100\n",
      "\n",
      "Epoch 14/100\n",
      "  Train Loss: 0.5556 | Train Acc: 0.7081\n",
      "  Val Loss: 0.8563 | Val Acc: 0.5854\n",
      "  LR: 0.000100\n",
      "\n",
      "Epoch 15/100\n",
      "  Train Loss: 0.5839 | Train Acc: 0.6770\n",
      "  Val Loss: 0.8907 | Val Acc: 0.4878\n",
      "  LR: 0.000100\n",
      "\n",
      "[LR Scheduler] Reduced LR: 0.000100 -> 0.000050\n",
      "Epoch 16/100\n",
      "  Train Loss: 0.5417 | Train Acc: 0.7329\n",
      "  Val Loss: 1.0141 | Val Acc: 0.6098\n",
      "  LR: 0.000050\n",
      "\n",
      "Epoch 17/100\n",
      "  Train Loss: 0.5610 | Train Acc: 0.6708\n",
      "  Val Loss: 0.9417 | Val Acc: 0.5854\n",
      "  LR: 0.000050\n",
      "\n",
      "Epoch 18/100\n",
      "  Train Loss: 0.5096 | Train Acc: 0.7329\n",
      "  Val Loss: 0.9668 | Val Acc: 0.4878\n",
      "  LR: 0.000050\n",
      "\n",
      "Epoch 19/100\n",
      "  Train Loss: 0.4395 | Train Acc: 0.8137\n",
      "  Val Loss: 1.1041 | Val Acc: 0.4634\n",
      "  LR: 0.000050\n",
      "\n",
      "Epoch 20/100\n",
      "  Train Loss: 0.4790 | Train Acc: 0.7950\n",
      "  Val Loss: 1.1007 | Val Acc: 0.4878\n",
      "  LR: 0.000050\n",
      "\n",
      "Epoch 21/100\n",
      "  Train Loss: 0.4185 | Train Acc: 0.8509\n",
      "  Val Loss: 1.0272 | Val Acc: 0.5122\n",
      "  LR: 0.000050\n",
      "\n",
      "[LR Scheduler] Reduced LR: 0.000050 -> 0.000025\n",
      "Epoch 22/100\n",
      "  Train Loss: 0.4247 | Train Acc: 0.8323\n",
      "  Val Loss: 1.0068 | Val Acc: 0.5854\n",
      "  LR: 0.000025\n",
      "\n",
      "Epoch 23/100\n",
      "  Train Loss: 0.3868 | Train Acc: 0.8571\n",
      "  Val Loss: 1.1218 | Val Acc: 0.4146\n",
      "  LR: 0.000025\n",
      "\n",
      "Epoch 24/100\n",
      "  Train Loss: 0.4024 | Train Acc: 0.8199\n",
      "  Val Loss: 1.1409 | Val Acc: 0.5122\n",
      "  LR: 0.000025\n",
      "\n",
      "Epoch 25/100\n",
      "  Train Loss: 0.3639 | Train Acc: 0.8696\n",
      "  Val Loss: 1.1355 | Val Acc: 0.5122\n",
      "  LR: 0.000025\n",
      "\n",
      "Early stopping triggered after 25 epochs\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Professional Plantar Pressure Analysis System\n",
    "==============================================\n",
    "\n",
    "This system provides three approaches for analyzing plantar pressure maps:\n",
    "1. Supervised Learning (when labels are available)\n",
    "2. Unsupervised Learning (anomaly detection & clustering)\n",
    "3. Interactive Labeling Tool\n",
    "\n",
    "Author: Research Project\n",
    "Date: 2025\n",
    "\n",
    "CHANGES FOR STABILITY:\n",
    "- Dataset __getitem__ hardened with try/except and traceback logging.\n",
    "- Pillow truncated images allowed.\n",
    "- DataLoader uses num_workers=0 and pin_memory=False (safe for GUI/Windows).\n",
    "- Train/val datasets use separate transforms via Subset objects.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import threading\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image, ImageTk, ImageFile\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox, ttk\n",
    "\n",
    "# Allow loading truncated images gracefully\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Compatibility for Pillow resampling\n",
    "try:\n",
    "    RESAMPLE_LANCZOS = Image.Resampling.LANCZOS\n",
    "except Exception:\n",
    "    RESAMPLE_LANCZOS = Image.LANCZOS\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Centralized configuration management\"\"\"\n",
    "    \n",
    "    # Paths (update DATASET_DIR to your real dataset path if needed)\n",
    "    DATASET_DIR = r\"C:\\Users\\koust\\AnaKonda\\FOOT_PLANTAR_CLASSIFICATION\\Dataset\"\n",
    "    LABELS_CSV = \"plantar_labels.csv\"\n",
    "    MODEL_SAVE_PATH = \"models/plantar_model.pth\"\n",
    "    RESULTS_DIR = \"results\"\n",
    "    FEATURES_DIR = \"features\"\n",
    "    \n",
    "    # Model settings\n",
    "    IMAGE_SIZE = (224, 224)  # Standard ImageNet size\n",
    "    BATCH_SIZE = 16\n",
    "    EPOCHS = 100\n",
    "    LEARNING_RATE = 0.0001\n",
    "    EARLY_STOPPING_PATIENCE = 15\n",
    "    \n",
    "    # Device configuration\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Feature extraction\n",
    "    FEATURE_METHODS = ['pressure_stats', 'pressure_distribution', 'asymmetry', 'cog']\n",
    "    \n",
    "    # Clustering\n",
    "    N_CLUSTERS = 3  # Can be adjusted\n",
    "    \n",
    "    @classmethod\n",
    "    def create_directories(cls):\n",
    "        \"\"\"Create necessary directories\"\"\"\n",
    "        for dir_path in [cls.RESULTS_DIR, cls.FEATURES_DIR, 'models', 'logs']:\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE EXTRACTION\n",
    "# =============================================================================\n",
    "\n",
    "class PressureMapFeatureExtractor:\n",
    "    \"\"\"Extract meaningful features from plantar pressure maps\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_names = []\n",
    "    \n",
    "    def extract_all_features(self, image_path):\n",
    "        \"\"\"Extract comprehensive features from pressure map\"\"\"\n",
    "        try:\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Could not load image: {image_path}\")\n",
    "            \n",
    "            # Convert to grayscale for pressure analysis\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            features = {}\n",
    "            features.update(self._extract_pressure_statistics(gray))\n",
    "            features.update(self._extract_pressure_distribution(gray))\n",
    "            features.update(self._extract_spatial_features(gray))\n",
    "            features.update(self._extract_asymmetry_features(image))\n",
    "            \n",
    "            return features\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting features from {image_path}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "    \n",
    "    def _extract_pressure_statistics(self, gray_image):\n",
    "        \"\"\"Basic statistical features of pressure\"\"\"\n",
    "        features = {\n",
    "            'mean_pressure': float(np.mean(gray_image)),\n",
    "            'std_pressure': float(np.std(gray_image)),\n",
    "            'max_pressure': float(np.max(gray_image)),\n",
    "            'min_pressure': float(np.min(gray_image)),\n",
    "            'median_pressure': float(np.median(gray_image)),\n",
    "            'pressure_range': float(np.max(gray_image) - np.min(gray_image)),\n",
    "        }\n",
    "        return features\n",
    "    \n",
    "    def _extract_pressure_distribution(self, gray_image):\n",
    "        \"\"\"Pressure distribution features\"\"\"\n",
    "        hist, _ = np.histogram(gray_image, bins=10, range=(0, 255))\n",
    "        hist = hist.astype(float)\n",
    "        if hist.sum() > 0:\n",
    "            hist = hist / hist.sum()  # Normalize\n",
    "        else:\n",
    "            hist = np.zeros_like(hist, dtype=float)\n",
    "        \n",
    "        features = {f'pressure_bin_{i}': float(val) for i, val in enumerate(hist)}\n",
    "        \n",
    "        # High pressure area\n",
    "        high_pressure_threshold = np.percentile(gray_image, 75)\n",
    "        high_pressure_area = float(np.sum(gray_image > high_pressure_threshold) / gray_image.size)\n",
    "        features['high_pressure_area_ratio'] = high_pressure_area\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _extract_spatial_features(self, gray_image):\n",
    "        \"\"\"Spatial distribution features\"\"\"\n",
    "        # Center of gravity\n",
    "        y_coords, x_coords = np.indices(gray_image.shape)\n",
    "        total_pressure = float(np.sum(gray_image))\n",
    "        \n",
    "        if total_pressure > 0:\n",
    "            cog_x = float(np.sum(x_coords * gray_image) / total_pressure)\n",
    "            cog_y = float(np.sum(y_coords * gray_image) / total_pressure)\n",
    "        else:\n",
    "            cog_x, cog_y = float(gray_image.shape[1] / 2), float(gray_image.shape[0] / 2)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        features = {\n",
    "            'cog_x_normalized': float(cog_x / gray_image.shape[1]),\n",
    "            'cog_y_normalized': float(cog_y / gray_image.shape[0]),\n",
    "        }\n",
    "        \n",
    "        # Contact area\n",
    "        contact_threshold = float(np.mean(gray_image) + np.std(gray_image))\n",
    "        contact_area = float(np.sum(gray_image > contact_threshold) / gray_image.size)\n",
    "        features['contact_area_ratio'] = contact_area\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _extract_asymmetry_features(self, image):\n",
    "        \"\"\"Extract left-right asymmetry features\"\"\"\n",
    "        h, w = image.shape[:2]\n",
    "        left_half = image[:, :w//2]\n",
    "        right_half = image[:, w//2:]\n",
    "        \n",
    "        # Flip right half for comparison\n",
    "        right_flipped = cv2.flip(right_half, 1)\n",
    "        \n",
    "        # Ensure same size\n",
    "        min_width = min(left_half.shape[1], right_flipped.shape[1])\n",
    "        left_half = left_half[:, :min_width]\n",
    "        right_flipped = right_flipped[:, :min_width]\n",
    "        \n",
    "        # Calculate asymmetry\n",
    "        diff = cv2.absdiff(left_half, right_flipped)\n",
    "        asymmetry_score = float(np.mean(diff) / 255.0) if diff.size else 0.0\n",
    "        \n",
    "        features = {\n",
    "            'lr_asymmetry': asymmetry_score,\n",
    "            'left_mean_pressure': float(np.mean(left_half)) if left_half.size else 0.0,\n",
    "            'right_mean_pressure': float(np.mean(right_half)) if right_half.size else 0.0,\n",
    "        }\n",
    "        \n",
    "        return features\n",
    "\n",
    "# =============================================================================\n",
    "# ADVANCED CNN MODEL\n",
    "# =============================================================================\n",
    "\n",
    "class ImprovedPlantarCNN(nn.Module):\n",
    "    \"\"\"Professional CNN with transfer learning and attention\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=2, pretrained=True):\n",
    "        super(ImprovedPlantarCNN, self).__init__()\n",
    "        \n",
    "        # Use ResNet18 as backbone\n",
    "        self.backbone = models.resnet18(pretrained=pretrained)\n",
    "        \n",
    "        # Modify first conv layer to accept our input\n",
    "        self.backbone.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, \n",
    "                                        padding=3, bias=False)\n",
    "        \n",
    "        # Get feature dimension\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        \n",
    "        # Replace final layer with custom classifier\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# =============================================================================\n",
    "# DATASET WITH AUGMENTATION (HARDENED)\n",
    "# =============================================================================\n",
    "\n",
    "class PlantarDataset(Dataset):\n",
    "    \"\"\"Enhanced dataset with proper augmentation and robust __getitem__\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, transform=None, augment=False):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        self.label_map = {'healthy': 0, 'unhealthy': 1, 'normal': 0, 'abnormal': 1}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx]['image_path']\n",
    "        label_str = self.df.iloc[idx]['label']\n",
    "        \n",
    "        try:\n",
    "            # Try loading image; catch and handle errors so DataLoader workers don't crash\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"[Dataset] Error loading image at idx={idx}, path={img_path}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            # Return a safe black image plus default label\n",
    "            image = Image.new('RGB', Config.IMAGE_SIZE, color='black')\n",
    "            label_str = 'healthy'\n",
    "        \n",
    "        label = self.label_map.get(str(label_str).lower(), 0)\n",
    "        \n",
    "        # Apply transform with protection\n",
    "        if self.transform:\n",
    "            try:\n",
    "                image = self.transform(image)\n",
    "            except Exception as e:\n",
    "                print(f\"[Dataset] Transform failed for idx={idx}, path={img_path}: {e}\")\n",
    "                traceback.print_exc()\n",
    "                # Default fallback tensor\n",
    "                image = transforms.ToTensor()(Image.new('RGB', Config.IMAGE_SIZE, color='black'))\n",
    "        \n",
    "        return image, label, img_path\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING ENGINE (backwards-compatible scheduler)\n",
    "# =============================================================================\n",
    "\n",
    "class TrainingEngine:\n",
    "    \"\"\"Professional training with all best practices (backwards-compatible scheduler)\"\"\"\n",
    "    \n",
    "    def __init__(self, model, train_loader, val_loader, device):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        \n",
    "        # Optimizer with weight decay\n",
    "        self.optimizer = optim.AdamW(\n",
    "            self.model.parameters(), \n",
    "            lr=Config.LEARNING_RATE,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        \n",
    "        # Loss function with class weights if imbalanced\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Learning rate scheduler: try verbose=True, fall back if not supported\n",
    "        try:\n",
    "            # Newer PyTorch supports verbose\n",
    "            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "            )\n",
    "            self._scheduler_supports_verbose = True\n",
    "        except TypeError:\n",
    "            # Older PyTorch -> create without verbose\n",
    "            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer, mode='min', factor=0.5, patience=5\n",
    "            )\n",
    "            self._scheduler_supports_verbose = False\n",
    "        \n",
    "        # Keep track of last LR so we can print a message when LR changes (for older torch)\n",
    "        self._last_lr = float(self.optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        # Early stopping\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.patience_counter = 0\n",
    "        \n",
    "        # History\n",
    "        self.history = {\n",
    "            'train_loss': [], 'val_loss': [], \n",
    "            'train_acc': [], 'val_acc': [],\n",
    "            'learning_rates': []\n",
    "        }\n",
    "\n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels, _ in self.train_loader:\n",
    "            images, labels = images.to(self.device), labels.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(self.train_loader.dataset) if len(self.train_loader.dataset)>0 else 0.0\n",
    "        epoch_acc = correct / total if total > 0 else 0.0\n",
    "        \n",
    "        return epoch_loss, epoch_acc\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"Validate model\"\"\"\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels, _ in self.val_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_loss = running_loss / len(self.val_loader.dataset) if len(self.val_loader.dataset)>0 else float('inf')\n",
    "        val_acc = correct / total if total>0 else 0.0\n",
    "        \n",
    "        return val_loss, val_acc, all_preds, all_labels\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        \"\"\"Full training loop\"\"\"\n",
    "        print(f\"\\nTraining on {self.device}\")\n",
    "        print(f\"Train size: {len(self.train_loader.dataset)}\")\n",
    "        print(f\"Val size: {len(self.val_loader.dataset)}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss, train_acc = self.train_epoch()\n",
    "            val_loss, val_acc, _, _ = self.validate()\n",
    "            \n",
    "            # Update scheduler with validation loss\n",
    "            # ReduceLROnPlateau expects the \"metric\" (val_loss) argument\n",
    "            self.scheduler.step(val_loss)\n",
    "            current_lr = float(self.optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "            # If verbose wasn't supported, print a message when LR actually changes\n",
    "            if not self._scheduler_supports_verbose and current_lr < self._last_lr:\n",
    "                print(f\"[LR Scheduler] Reduced LR: {self._last_lr:.6f} -> {current_lr:.6f}\")\n",
    "            self._last_lr = current_lr\n",
    "            \n",
    "            # Save history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_acc'].append(val_acc)\n",
    "            self.history['learning_rates'].append(current_lr)\n",
    "            \n",
    "            print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "            print(f'  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}')\n",
    "            print(f'  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n",
    "            print(f'  LR: {current_lr:.6f}')\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.patience_counter = 0\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_acc': val_acc,\n",
    "                }, Config.MODEL_SAVE_PATH)\n",
    "                print(f'  ✓ Model saved (Val Loss improved)')\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                if self.patience_counter >= Config.EARLY_STOPPING_PATIENCE:\n",
    "                    print(f'\\nEarly stopping triggered after {epoch+1} epochs')\n",
    "                    break\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        return self.history\n",
    "\n",
    "# =============================================================================\n",
    "# UNSUPERVISED LEARNING\n",
    "# =============================================================================\n",
    "\n",
    "class UnsupervisedAnalyzer:\n",
    "    \"\"\"Unsupervised learning for pattern discovery\"\"\"\n",
    "    \n",
    "    def __init__(self, image_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.feature_extractor = PressureMapFeatureExtractor()\n",
    "        self.features_df = None\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def extract_features_from_dataset(self):\n",
    "        \"\"\"Extract features from all images\"\"\"\n",
    "        image_files = [f for f in os.listdir(self.image_dir) \n",
    "                      if f.lower().endswith(('.jpg', '.jpeg', '.png', '.tiff', '.bmp'))]\n",
    "        \n",
    "        features_list = []\n",
    "        valid_files = []\n",
    "        \n",
    "        print(f\"Extracting features from {len(image_files)} images...\")\n",
    "        for img_file in image_files:\n",
    "            img_path = os.path.join(self.image_dir, img_file)\n",
    "            features = self.feature_extractor.extract_all_features(img_path)\n",
    "            if features is not None:\n",
    "                features['filename'] = img_file\n",
    "                features_list.append(features)\n",
    "                valid_files.append(img_file)\n",
    "        \n",
    "        self.features_df = pd.DataFrame(features_list)\n",
    "        print(f\"Extracted features from {len(features_list)} images\")\n",
    "        \n",
    "        # Save features\n",
    "        features_path = os.path.join(Config.FEATURES_DIR, 'extracted_features.csv')\n",
    "        try:\n",
    "            self.features_df.to_csv(features_path, index=False, encoding='utf-8-sig')\n",
    "        except Exception:\n",
    "            self.features_df.to_csv(features_path, index=False)\n",
    "        print(f\"Features saved to {features_path}\")\n",
    "        \n",
    "        return self.features_df\n",
    "    \n",
    "    def perform_clustering(self, n_clusters=3, method='kmeans'):\n",
    "        \"\"\"Perform clustering analysis\"\"\"\n",
    "        if self.features_df is None:\n",
    "            print(\"No features available. Extract features first.\")\n",
    "            return None\n",
    "        \n",
    "        # Prepare features (exclude filename)\n",
    "        feature_cols = [col for col in self.features_df.columns if col != 'filename']\n",
    "        X = self.features_df[feature_cols].values\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Clustering\n",
    "        if method == 'kmeans':\n",
    "            clusterer = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        elif method == 'dbscan':\n",
    "            clusterer = DBSCAN(eps=0.5, min_samples=5)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "        \n",
    "        labels = clusterer.fit_predict(X_scaled)\n",
    "        \n",
    "        # Add labels to dataframe\n",
    "        self.features_df['cluster'] = labels\n",
    "        \n",
    "        # Calculate silhouette score if more than 1 cluster\n",
    "        if len(set(labels)) > 1:\n",
    "            try:\n",
    "                sil_score = silhouette_score(X_scaled, labels)\n",
    "                print(f\"Silhouette Score: {sil_score:.3f}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        # Save clustering results\n",
    "        results_path = os.path.join(Config.RESULTS_DIR, f'clustering_{method}_results.csv')\n",
    "        try:\n",
    "            self.features_df.to_csv(results_path, index=False, encoding='utf-8-sig')\n",
    "        except Exception:\n",
    "            self.features_df.to_csv(results_path, index=False)\n",
    "        print(f\"Clustering results saved to {results_path}\")\n",
    "        \n",
    "        return labels, X_scaled\n",
    "    \n",
    "    def visualize_clusters(self, X_scaled, labels):\n",
    "        \"\"\"Visualize clustering results using PCA\"\"\"\n",
    "        # PCA for visualization\n",
    "        pca = PCA(n_components=2)\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Scatter plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, \n",
    "                            cmap='viridis', alpha=0.6, edgecolors='k')\n",
    "        plt.colorbar(scatter, label='Cluster')\n",
    "        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "        plt.title('Cluster Visualization (PCA)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Cluster distribution\n",
    "        plt.subplot(1, 2, 2)\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        plt.bar(unique, counts, color='skyblue', edgecolor='black')\n",
    "        plt.xlabel('Cluster')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Cluster Distribution')\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_path = os.path.join(Config.RESULTS_DIR, 'clustering_visualization.png')\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Visualization saved to {plot_path}\")\n",
    "        \n",
    "        return X_pca\n",
    "\n",
    "# =============================================================================\n",
    "# INTERPRETABILITY & VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "class GradCAM:\n",
    "    \"\"\"Gradient-weighted Class Activation Mapping for interpretability\"\"\"\n",
    "    \n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        # Register hooks\n",
    "        target_layer.register_forward_hook(self.save_activation)\n",
    "        target_layer.register_backward_hook(self.save_gradient)\n",
    "    \n",
    "    def save_activation(self, module, input, output):\n",
    "        self.activations = output.detach()\n",
    "    \n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0].detach()\n",
    "    \n",
    "    def generate_cam(self, input_image, target_class=None):\n",
    "        \"\"\"Generate Class Activation Map\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = self.model(input_image)\n",
    "        \n",
    "        if target_class is None:\n",
    "            target_class = output.argmax(dim=1).item()\n",
    "        \n",
    "        # Backward pass\n",
    "        self.model.zero_grad()\n",
    "        target = output[0, target_class]\n",
    "        target.backward()\n",
    "        \n",
    "        # Get weights\n",
    "        weights = self.gradients.mean(dim=(2, 3), keepdim=True)\n",
    "        \n",
    "        # Weighted combination\n",
    "        cam = (weights * self.activations).sum(dim=1, keepdim=True)\n",
    "        cam = torch.relu(cam)\n",
    "        \n",
    "        # Normalize\n",
    "        cam = cam - cam.min()\n",
    "        cam = cam / (cam.max() + 1e-8)\n",
    "        \n",
    "        return cam.squeeze().cpu().numpy(), target_class\n",
    "\n",
    "# =============================================================================\n",
    "# INTERACTIVE LABELING TOOL (UNCHANGED)\n",
    "# =============================================================================\n",
    "\n",
    "class LabelingTool:\n",
    "    \"\"\"Interactive tool for manual labeling (robust image display & resizing)\"\"\"\n",
    "    \n",
    "    def __init__(self, root, image_dir):\n",
    "        self.root = root\n",
    "        self.image_dir = image_dir\n",
    "        self.current_idx = 0\n",
    "        self.labels = {}\n",
    "        self.current_image = None   # PIL Image of currently loaded image (original)\n",
    "        self.current_photo = None   # ImageTk.PhotoImage reference\n",
    "        \n",
    "        # Get image files\n",
    "        self.image_files = [f for f in os.listdir(image_dir)\n",
    "                          if f.lower().endswith(('.jpg', '.jpeg', '.png', '.tiff', '.bmp'))]\n",
    "        self.image_files.sort()\n",
    "        \n",
    "        # Load existing labels if any\n",
    "        if os.path.exists(Config.LABELS_CSV):\n",
    "            try:\n",
    "                df = pd.read_csv(Config.LABELS_CSV)\n",
    "                self.labels = dict(zip(df['image_path'], df['label']))\n",
    "                print(f\"Loaded {len(self.labels)} existing labels\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        self.create_widgets()\n",
    "        # Wait until the window is visible to load the first image\n",
    "        self.root.after(100, lambda: self.load_image(self.current_idx))\n",
    "    \n",
    "    def create_widgets(self):\n",
    "        \"\"\"Create labeling interface\"\"\"\n",
    "        self.root.title(\"Plantar Pressure Image Labeling Tool\")\n",
    "        self.root.geometry(\"900x700\")\n",
    "        \n",
    "        # Top info bar\n",
    "        info_frame = ttk.Frame(self.root)\n",
    "        info_frame.pack(fill=tk.X, padx=10, pady=5)\n",
    "        \n",
    "        self.progress_label = ttk.Label(info_frame, \n",
    "                                       text=f\"Image 0/{len(self.image_files)}\", \n",
    "                                       font=('Arial', 12, 'bold'))\n",
    "        self.progress_label.pack(side=tk.LEFT)\n",
    "        \n",
    "        labeled_count = len([v for v in self.labels.values() if v])\n",
    "        self.labeled_label = ttk.Label(info_frame, \n",
    "                                       text=f\"Labeled: {labeled_count}\", \n",
    "                                       font=('Arial', 12))\n",
    "        self.labeled_label.pack(side=tk.RIGHT)\n",
    "        \n",
    "        # Image display\n",
    "        img_frame = ttk.LabelFrame(self.root, text=\"Plantar Pressure Map\")\n",
    "        img_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)\n",
    "        \n",
    "        # Create canvas with a default size; allow it to expand\n",
    "        self.canvas = tk.Canvas(img_frame, bg='black', highlightthickness=1, highlightbackground='#cccccc')\n",
    "        self.canvas.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        # Bind resize event to redraw correctly\n",
    "        self.canvas.bind(\"<Configure>\", self._on_canvas_configure)\n",
    "        \n",
    "        # Filename display\n",
    "        self.filename_label = ttk.Label(self.root, text=\"\", \n",
    "                                       font=('Arial', 10))\n",
    "        self.filename_label.pack(pady=2)\n",
    "        \n",
    "        # Labeling buttons\n",
    "        button_frame = ttk.LabelFrame(self.root, text=\"Classification\")\n",
    "        button_frame.pack(fill=tk.X, padx=10, pady=5)\n",
    "        \n",
    "        # ttk.Button doesn't accept 'height' param in same way as tk.Button; avoid non-portable options\n",
    "        btn_frame_inner = ttk.Frame(button_frame)\n",
    "        btn_frame_inner.pack(pady=10)\n",
    "        \n",
    "        ttk.Button(btn_frame_inner, text=\"Normal\", command=lambda: self.label_current('healthy')).pack(side=tk.LEFT, padx=10)\n",
    "        ttk.Button(btn_frame_inner, text=\"Abnormal\", command=lambda: self.label_current('unhealthy')).pack(side=tk.LEFT, padx=10)\n",
    "        ttk.Button(btn_frame_inner, text=\"Skip\", command=self.skip_image).pack(side=tk.LEFT, padx=10)\n",
    "        \n",
    "        # Navigation\n",
    "        nav_frame = ttk.Frame(self.root)\n",
    "        nav_frame.pack(fill=tk.X, padx=10, pady=5)\n",
    "        \n",
    "        ttk.Button(nav_frame, text=\"◄ Previous\", command=self.prev_image).pack(side=tk.LEFT, padx=5)\n",
    "        ttk.Button(nav_frame, text=\"Next ►\", command=self.next_image).pack(side=tk.LEFT, padx=5)\n",
    "        ttk.Button(nav_frame, text=\"Save & Exit\", command=self.save_and_exit).pack(side=tk.RIGHT, padx=5)\n",
    "        \n",
    "        # Keyboard shortcuts\n",
    "        self.root.bind('1', lambda e: self.label_current('healthy'))\n",
    "        self.root.bind('2', lambda e: self.label_current('unhealthy'))\n",
    "        self.root.bind('<space>', lambda e: self.skip_image())\n",
    "        self.root.bind('<Left>', lambda e: self.prev_image())\n",
    "        self.root.bind('<Right>', lambda e: self.next_image())\n",
    "    \n",
    "    def _on_canvas_configure(self, event):\n",
    "        \"\"\"Redraw current image when the canvas size changes\"\"\"\n",
    "        # If we already have an image loaded, rescale and redraw it\n",
    "        if self.current_image is not None:\n",
    "            self._display_image_on_canvas(self.current_image)\n",
    "    \n",
    "    def load_image(self, idx):\n",
    "        \"\"\"Load and display image at index idx\"\"\"\n",
    "        if idx < 0 or idx >= len(self.image_files):\n",
    "            return\n",
    "        \n",
    "        self.current_idx = idx\n",
    "        filename = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, filename)\n",
    "        \n",
    "        try:\n",
    "            # Load original PIL image and keep it for resizing/redraw\n",
    "            pil_image = Image.open(img_path).convert('RGB')\n",
    "            self.current_image = pil_image  # store original image\n",
    "            \n",
    "            # Immediately attempt to display (display will scale to current canvas size)\n",
    "            self._display_image_on_canvas(pil_image)\n",
    "            \n",
    "            # Update labels\n",
    "            self.progress_label.config(text=f\"Image {idx+1}/{len(self.image_files)}\")\n",
    "            \n",
    "            current_label = self.labels.get(img_path, \"Not labeled\")\n",
    "            self.filename_label.config(text=f\"{filename} | Current: {current_label}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Failed to load image: {e}\")\n",
    "            self.current_image = None\n",
    "            self.canvas.delete(\"all\")\n",
    "    \n",
    "    def _display_image_on_canvas(self, pil_image):\n",
    "        \"\"\"Scale pil_image to the current canvas size and display it\"\"\"\n",
    "        # Compute available drawing area (subtract small padding)\n",
    "        canvas_width = max(1, self.canvas.winfo_width())\n",
    "        canvas_height = max(1, self.canvas.winfo_height())\n",
    "        \n",
    "        # If canvas is not yet realized (very small), schedule re-draw soon\n",
    "        if canvas_width < 10 or canvas_height < 10:\n",
    "            # Try again shortly after mainloop is idle\n",
    "            self.root.after(100, lambda: self._display_image_on_canvas(pil_image))\n",
    "            return\n",
    "        \n",
    "        # Determine max allowed size (leave small margins)\n",
    "        max_w = max(1, canvas_width - 10)\n",
    "        max_h = max(1, canvas_height - 10)\n",
    "        \n",
    "        # Create a copy to avoid modifying original\n",
    "        image_copy = pil_image.copy()\n",
    "        image_copy.thumbnail((max_w, max_h), RESAMPLE_LANCZOS)\n",
    "        \n",
    "        # Create PhotoImage and keep a reference so it is not garbage-collected\n",
    "        self.current_photo = ImageTk.PhotoImage(image_copy)\n",
    "        \n",
    "        # Clear canvas and draw image anchored at top-left (north-west)\n",
    "        self.canvas.delete(\"all\")\n",
    "        self.canvas.create_image(0, 0, image=self.current_photo, anchor='nw')\n",
    "    \n",
    "    def label_current(self, label):\n",
    "        \"\"\"Label current image\"\"\"\n",
    "        filename = self.image_files[self.current_idx]\n",
    "        img_path = os.path.join(self.image_dir, filename)\n",
    "        self.labels[img_path] = label\n",
    "        \n",
    "        # Update labeled count\n",
    "        labeled_count = len([v for v in self.labels.values() if v])\n",
    "        self.labeled_label.config(text=f\"Labeled: {labeled_count}\")\n",
    "        \n",
    "        # Move to next\n",
    "        self.next_image()\n",
    "    \n",
    "    def skip_image(self):\n",
    "        \"\"\"Skip current image\"\"\"\n",
    "        self.next_image()\n",
    "    \n",
    "    def next_image(self):\n",
    "        \"\"\"Go to next image\"\"\"\n",
    "        if self.current_idx < len(self.image_files) - 1:\n",
    "            self.load_image(self.current_idx + 1)\n",
    "    \n",
    "    def prev_image(self):\n",
    "        \"\"\"Go to previous image\"\"\"\n",
    "        if self.current_idx > 0:\n",
    "            self.load_image(self.current_idx - 1)\n",
    "    \n",
    "    def save_and_exit(self):\n",
    "        \"\"\"Save labels and exit\"\"\"\n",
    "        if not self.labels:\n",
    "            messagebox.showwarning(\"Warning\", \"No labels to save!\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrame\n",
    "        data = []\n",
    "        for img_path, label in self.labels.items():\n",
    "            data.append({'image_path': img_path, 'label': label})\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        try:\n",
    "            df.to_csv(Config.LABELS_CSV, index=False, encoding='utf-8-sig')\n",
    "        except Exception:\n",
    "            df.to_csv(Config.LABELS_CSV, index=False)\n",
    "        \n",
    "        messagebox.showinfo(\"Success\", f\"Saved {len(self.labels)} labels to {Config.LABELS_CSV}\")\n",
    "        self.root.destroy()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN APPLICATION\n",
    "# =============================================================================\n",
    "\n",
    "class ProfessionalPlantarApp:\n",
    "    \"\"\"Main application with all features\"\"\"\n",
    "    \n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Professional Plantar Pressure Analysis System\")\n",
    "        self.root.geometry(\"1200x800\")\n",
    "        \n",
    "        Config.create_directories()\n",
    "        \n",
    "        # Initialize components\n",
    "        self.model = None\n",
    "        self.unsupervised_analyzer = UnsupervisedAnalyzer(Config.DATASET_DIR)\n",
    "        self.feature_extractor = PressureMapFeatureExtractor()\n",
    "        \n",
    "        # Transform for prediction\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(Config.IMAGE_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.create_gui()\n",
    "        self.load_model_if_exists()\n",
    "    \n",
    "    def create_gui(self):\n",
    "        \"\"\"Create main interface\"\"\"\n",
    "        # Menu bar\n",
    "        menubar = tk.Menu(self.root)\n",
    "        self.root.config(menu=menubar)\n",
    "        \n",
    "        # File menu\n",
    "        file_menu = tk.Menu(menubar, tearoff=0)\n",
    "        menubar.add_cascade(label=\"File\", menu=file_menu)\n",
    "        file_menu.add_command(label=\"Label Images\", command=self.open_labeling_tool)\n",
    "        file_menu.add_separator()\n",
    "        file_menu.add_command(label=\"Exit\", command=self.root.quit)\n",
    "        \n",
    "        # Analysis menu\n",
    "        analysis_menu = tk.Menu(menubar, tearoff=0)\n",
    "        menubar.add_cascade(label=\"Analysis\", menu=analysis_menu)\n",
    "        analysis_menu.add_command(label=\"Extract Features\", \n",
    "                                 command=self.extract_features)\n",
    "        analysis_menu.add_command(label=\"Cluster Analysis\", \n",
    "                                 command=self.run_clustering)\n",
    "        analysis_menu.add_command(label=\"View Results\", \n",
    "                                 command=self.view_results)\n",
    "        \n",
    "        # Model menu\n",
    "        model_menu = tk.Menu(menubar, tearoff=0)\n",
    "        menubar.add_cascade(label=\"Model\", menu=model_menu)\n",
    "        model_menu.add_command(label=\"Train Model\", command=self.train_model)\n",
    "        model_menu.add_command(label=\"Load Model\", command=self.load_model_if_exists)\n",
    "        \n",
    "        # Main notebook\n",
    "        self.notebook = ttk.Notebook(self.root)\n",
    "        self.notebook.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "        \n",
    "        # Prediction tab\n",
    "        self.create_prediction_tab()\n",
    "        \n",
    "        # Analysis tab\n",
    "        self.create_analysis_tab()\n",
    "        \n",
    "        # Status bar\n",
    "        self.status_var = tk.StringVar(value=\"Ready\")\n",
    "        status_bar = ttk.Label(self.root, textvariable=self.status_var, \n",
    "                              relief=tk.SUNKEN, anchor=tk.W)\n",
    "        status_bar.pack(side=tk.BOTTOM, fill=tk.X)\n",
    "    \n",
    "    def create_prediction_tab(self):\n",
    "        \"\"\"Create prediction interface\"\"\"\n",
    "        pred_frame = ttk.Frame(self.notebook)\n",
    "        self.notebook.add(pred_frame, text=\"Image Prediction\")\n",
    "        \n",
    "        # Control panel\n",
    "        control_frame = ttk.LabelFrame(pred_frame, text=\"Controls\")\n",
    "        control_frame.pack(fill=tk.X, padx=10, pady=5)\n",
    "        \n",
    "        ttk.Button(control_frame, text=\"Load Image\", \n",
    "                  command=self.load_and_predict).pack(side=tk.LEFT, padx=5, pady=5)\n",
    "        ttk.Button(control_frame, text=\"Batch Predict\", \n",
    "                  command=self.batch_predict).pack(side=tk.LEFT, padx=5, pady=5)\n",
    "        \n",
    "        # Content area (split)\n",
    "        content_frame = ttk.Frame(pred_frame)\n",
    "        content_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)\n",
    "        \n",
    "        # Left: Image display\n",
    "        left_frame = ttk.LabelFrame(content_frame, text=\"Image\")\n",
    "        left_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=5)\n",
    "        \n",
    "        self.image_canvas = tk.Canvas(left_frame, bg='black')\n",
    "        self.image_canvas.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        self.image_canvas.bind(\"<Configure>\", lambda e: None)  # keep to allow future resizing behavior\n",
    "        \n",
    "        # Right: Results\n",
    "        right_frame = ttk.LabelFrame(content_frame, text=\"Results\")\n",
    "        right_frame.pack(side=tk.RIGHT, fill=tk.BOTH, expand=True, padx=5)\n",
    "        \n",
    "        # Prediction result\n",
    "        result_frame = ttk.Frame(right_frame)\n",
    "        result_frame.pack(fill=tk.X, pady=10, padx=10)\n",
    "        \n",
    "        ttk.Label(result_frame, text=\"Classification:\", \n",
    "                 font=('Arial', 11, 'bold')).pack(anchor=tk.W)\n",
    "        self.pred_result_var = tk.StringVar(value=\"No prediction\")\n",
    "        self.pred_result_label = ttk.Label(result_frame, \n",
    "                                          textvariable=self.pred_result_var,\n",
    "                                          font=('Arial', 14, 'bold'))\n",
    "        self.pred_result_label.pack(anchor=tk.W, pady=5)\n",
    "        \n",
    "        # Confidence\n",
    "        conf_frame = ttk.LabelFrame(right_frame, text=\"Confidence\")\n",
    "        conf_frame.pack(fill=tk.X, pady=10, padx=10)\n",
    "        \n",
    "        self.conf_text = tk.Text(conf_frame, height=3, font=('Consolas', 10))\n",
    "        self.conf_text.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        # Features\n",
    "        feat_frame = ttk.LabelFrame(right_frame, text=\"Extracted Features\")\n",
    "        feat_frame.pack(fill=tk.BOTH, expand=True, pady=10, padx=10)\n",
    "        \n",
    "        self.feat_text = tk.Text(feat_frame, font=('Consolas', 9))\n",
    "        feat_scrollbar = ttk.Scrollbar(feat_frame, command=self.feat_text.yview)\n",
    "        self.feat_text.configure(yscrollcommand=feat_scrollbar.set)\n",
    "        feat_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "        self.feat_text.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "    \n",
    "    def create_analysis_tab(self):\n",
    "        \"\"\"Create analysis interface\"\"\"\n",
    "        analysis_frame = ttk.Frame(self.notebook)\n",
    "        self.notebook.add(analysis_frame, text=\"Dataset Analysis\")\n",
    "        \n",
    "        # Analysis text area\n",
    "        self.analysis_text = tk.Text(analysis_frame, font=('Consolas', 10))\n",
    "        scrollbar = ttk.Scrollbar(analysis_frame, command=self.analysis_text.yview)\n",
    "        self.analysis_text.configure(yscrollcommand=scrollbar.set)\n",
    "        \n",
    "        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "        self.analysis_text.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "    \n",
    "    def open_labeling_tool(self):\n",
    "        \"\"\"Open labeling tool in new window\"\"\"\n",
    "        labeling_window = tk.Toplevel(self.root)\n",
    "        LabelingTool(labeling_window, Config.DATASET_DIR)\n",
    "    \n",
    "    def extract_features(self):\n",
    "        \"\"\"Extract features from all images\"\"\"\n",
    "        self.status_var.set(\"Extracting features...\")\n",
    "        self.root.update()\n",
    "        \n",
    "        def extract_thread():\n",
    "            try:\n",
    "                features_df = self.unsupervised_analyzer.extract_features_from_dataset()\n",
    "                \n",
    "                # Display summary\n",
    "                summary = f\"Feature Extraction Complete\\n\"\n",
    "                summary += f\"{'='*50}\\n\\n\"\n",
    "                summary += f\"Total images processed: {len(features_df)}\\n\"\n",
    "                summary += f\"Total features: {len(features_df.columns)-1}\\n\\n\"\n",
    "                summary += f\"Feature Statistics:\\n\"\n",
    "                summary += f\"{'-'*50}\\n\"\n",
    "                summary += features_df.describe().to_string()\n",
    "                \n",
    "                self.analysis_text.delete('1.0', tk.END)\n",
    "                self.analysis_text.insert('1.0', summary)\n",
    "                self.status_var.set(\"Feature extraction complete\")\n",
    "            except Exception as e:\n",
    "                self.status_var.set(f\"Error: {e}\")\n",
    "                messagebox.showerror(\"Error\", str(e))\n",
    "        \n",
    "        threading.Thread(target=extract_thread, daemon=True).start()\n",
    "    \n",
    "    def run_clustering(self):\n",
    "        \"\"\"Run clustering analysis\"\"\"\n",
    "        self.status_var.set(\"Running clustering...\")\n",
    "        self.root.update()\n",
    "        \n",
    "        def cluster_thread():\n",
    "            try:\n",
    "                labels, X_scaled = self.unsupervised_analyzer.perform_clustering(\n",
    "                    n_clusters=Config.N_CLUSTERS\n",
    "                )\n",
    "                \n",
    "                X_pca = self.unsupervised_analyzer.visualize_clusters(X_scaled, labels)\n",
    "                \n",
    "                # Display results\n",
    "                summary = f\"Clustering Analysis Complete\\n\"\n",
    "                summary += f\"{'='*50}\\n\\n\"\n",
    "                summary += f\"Method: K-Means\\n\"\n",
    "                summary += f\"Number of clusters: {Config.N_CLUSTERS}\\n\\n\"\n",
    "                summary += f\"Cluster Distribution:\\n\"\n",
    "                summary += f\"{'-'*50}\\n\"\n",
    "                \n",
    "                for cluster_id in range(Config.N_CLUSTERS):\n",
    "                    count = np.sum(labels == cluster_id)\n",
    "                    percentage = (count / len(labels)) * 100\n",
    "                    summary += f\"Cluster {cluster_id}: {count} images ({percentage:.1f}%)\\n\"\n",
    "                \n",
    "                summary += f\"\\n\\nInterpretation:\\n\"\n",
    "                summary += f\"{'-'*50}\\n\"\n",
    "                summary += f\"Images have been grouped into {Config.N_CLUSTERS} distinct patterns.\\n\"\n",
    "                summary += f\"Review 'clustering_kmeans_results.csv' to see which images\\n\"\n",
    "                summary += f\"belong to each cluster. This can help identify:\\n\"\n",
    "                summary += f\"  - Normal vs abnormal patterns\\n\"\n",
    "                summary += f\"  - Different types of gait abnormalities\\n\"\n",
    "                summary += f\"  - Outliers or unusual cases\\n\"\n",
    "                \n",
    "                self.analysis_text.delete('1.0', tk.END)\n",
    "                self.analysis_text.insert('1.0', summary)\n",
    "                self.status_var.set(\"Clustering complete - check results folder\")\n",
    "                \n",
    "                messagebox.showinfo(\"Success\", \n",
    "                                  \"Clustering complete! Check 'results' folder for visualizations.\")\n",
    "            except Exception as e:\n",
    "                self.status_var.set(f\"Error: {e}\")\n",
    "                messagebox.showerror(\"Error\", str(e))\n",
    "        \n",
    "        threading.Thread(target=cluster_thread, daemon=True).start()\n",
    "    \n",
    "    def view_results(self):\n",
    "        \"\"\"View analysis results\"\"\"\n",
    "        results_dir = Config.RESULTS_DIR\n",
    "        if os.path.exists(results_dir):\n",
    "            try:\n",
    "                os.startfile(results_dir)  # Windows\n",
    "            except Exception:\n",
    "                messagebox.showinfo(\"Info\", f\"Results path: {os.path.abspath(results_dir)}\")\n",
    "        else:\n",
    "            messagebox.showinfo(\"Info\", \"No results available yet\")\n",
    "    \n",
    "    def load_model_if_exists(self):\n",
    "        \"\"\"Load trained model if available\"\"\"\n",
    "        if os.path.exists(Config.MODEL_SAVE_PATH):\n",
    "            try:\n",
    "                self.model = ImprovedPlantarCNN().to(Config.DEVICE)\n",
    "                checkpoint = torch.load(Config.MODEL_SAVE_PATH, map_location=Config.DEVICE)\n",
    "                self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                self.model.eval()\n",
    "                self.status_var.set(f\"Model loaded successfully (Val Acc: {checkpoint.get('val_acc', 0):.2%})\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                self.status_var.set(f\"Error loading model: {e}\")\n",
    "                return False\n",
    "        else:\n",
    "            self.status_var.set(\"No trained model found\")\n",
    "            return False\n",
    "    \n",
    "    def train_model(self):\n",
    "        \"\"\"Train supervised model\"\"\"\n",
    "        if not os.path.exists(Config.LABELS_CSV):\n",
    "            messagebox.showerror(\"Error\", \n",
    "                               \"No labels file found! Please label images first.\")\n",
    "            return\n",
    "        \n",
    "        response = messagebox.askyesno(\"Train Model\", \n",
    "                                      \"This will start training. Continue?\")\n",
    "        if not response:\n",
    "            return\n",
    "        \n",
    "        self.status_var.set(\"Training started...\")\n",
    "        \n",
    "        def train_thread():\n",
    "            try:\n",
    "                # Data augmentation\n",
    "                train_transform = transforms.Compose([\n",
    "                    transforms.Resize(Config.IMAGE_SIZE),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomRotation(15),\n",
    "                    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                       std=[0.229, 0.224, 0.225])\n",
    "                ])\n",
    "                \n",
    "                val_transform = transforms.Compose([\n",
    "                    transforms.Resize(Config.IMAGE_SIZE),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                       std=[0.229, 0.224, 0.225])\n",
    "                ])\n",
    "                \n",
    "                # Read total examples\n",
    "                df = pd.read_csv(Config.LABELS_CSV)\n",
    "                total_len = len(df)\n",
    "                if total_len < 2:\n",
    "                    messagebox.showerror(\"Error\", \"Not enough labeled examples to train.\")\n",
    "                    self.status_var.set(\"Training aborted: insufficient data\")\n",
    "                    return\n",
    "                \n",
    "                # Create shuffled indices and split\n",
    "                train_size = int(0.8 * total_len)\n",
    "                indices = list(range(total_len))\n",
    "                np.random.seed(42)\n",
    "                np.random.shuffle(indices)\n",
    "                train_indices = indices[:train_size]\n",
    "                val_indices = indices[train_size:]\n",
    "                \n",
    "                # Create dataset objects with appropriate transforms and wrap in Subset\n",
    "                train_dataset = Subset(PlantarDataset(Config.LABELS_CSV, transform=train_transform), train_indices)\n",
    "                val_dataset = Subset(PlantarDataset(Config.LABELS_CSV, transform=val_transform), val_indices)\n",
    "                \n",
    "                # Loaders - USE num_workers=0 for GUI-safe behavior (avoids worker spawn in threads on Windows)\n",
    "                train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, \n",
    "                                        shuffle=True, num_workers=0, pin_memory=False)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE, \n",
    "                                      num_workers=0, pin_memory=False)\n",
    "                \n",
    "                # Model\n",
    "                model = ImprovedPlantarCNN(pretrained=True)\n",
    "                \n",
    "                # Training\n",
    "                engine = TrainingEngine(model, train_loader, val_loader, Config.DEVICE)\n",
    "                history = engine.train(Config.EPOCHS)\n",
    "                \n",
    "                self.status_var.set(\"Training complete!\")\n",
    "                messagebox.showinfo(\"Success\", \"Model training complete!\")\n",
    "                \n",
    "                # Load the trained model\n",
    "                self.load_model_if_exists()\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Log the exception and show user-friendly message\n",
    "                print(\"Training error:\", e)\n",
    "                traceback.print_exc()\n",
    "                self.status_var.set(f\"Training error: {e}\")\n",
    "                messagebox.showerror(\"Error\", str(e))\n",
    "        \n",
    "        # Run training in a thread (safe because num_workers=0). If you later want num_workers>0,\n",
    "        # run training in a separate process (multiprocessing) or from the main script.\n",
    "        threading.Thread(target=train_thread, daemon=True).start()\n",
    "    \n",
    "    def load_and_predict(self):\n",
    "        \"\"\"Load image and make prediction\"\"\"\n",
    "        if self.model is None:\n",
    "            messagebox.showerror(\"Error\", \"No model loaded!\")\n",
    "            return\n",
    "        \n",
    "        file_path = filedialog.askopenfilename(\n",
    "            filetypes=[(\"Image files\", \"*.jpg;*.jpeg;*.png;*.tiff;*.bmp\")]\n",
    "        )\n",
    "        if not file_path:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Load and display - robust resizing similar to labeling tool\n",
    "            image = Image.open(file_path).convert('RGB')\n",
    "            display_img = image.copy()\n",
    "            # scale to canvas size\n",
    "            canvas_width = max(1, self.image_canvas.winfo_width())\n",
    "            canvas_height = max(1, self.image_canvas.winfo_height())\n",
    "            if canvas_width > 10 and canvas_height > 10:\n",
    "                display_img.thumbnail((canvas_width-10, canvas_height-10), RESAMPLE_LANCZOS)\n",
    "            \n",
    "            photo = ImageTk.PhotoImage(display_img)\n",
    "            \n",
    "            self.image_canvas.delete(\"all\")\n",
    "            # Keep reference\n",
    "            self.image_canvas.image = photo\n",
    "            # Draw top-left\n",
    "            self.image_canvas.create_image(0, 0, image=photo, anchor='nw')\n",
    "            \n",
    "            # Predict\n",
    "            input_tensor = self.transform(image).unsqueeze(0).to(Config.DEVICE)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_tensor)\n",
    "                probabilities = torch.softmax(outputs, dim=1)[0]\n",
    "            \n",
    "            # Get results\n",
    "            classes = ['Normal (Healthy)', 'Abnormal (Unhealthy)']\n",
    "            pred_class = int(probabilities.argmax().item())\n",
    "            confidence = float(probabilities[pred_class].item() * 100)\n",
    "            \n",
    "            # Update UI\n",
    "            self.pred_result_var.set(classes[pred_class])\n",
    "            color = \"green\" if pred_class == 0 else \"red\"\n",
    "            self.pred_result_label.config(foreground=color)\n",
    "            \n",
    "            # Confidence details\n",
    "            conf_text = f\"Normal:   {probabilities[0].item()*100:.2f}%\\n\"\n",
    "            conf_text += f\"Abnormal: {probabilities[1].item()*100:.2f}%\\n\"\n",
    "            conf_text += f\"Confidence: {confidence:.2f}%\"\n",
    "            \n",
    "            self.conf_text.delete('1.0', tk.END)\n",
    "            self.conf_text.insert('1.0', conf_text)\n",
    "            \n",
    "            # Extract and display features\n",
    "            features = self.feature_extractor.extract_all_features(file_path)\n",
    "            if features:\n",
    "                feat_text = \"Key Features:\\n\" + \"=\"*40 + \"\\n\\n\"\n",
    "                for key, value in sorted(features.items()):\n",
    "                    if key != 'filename':\n",
    "                        try:\n",
    "                            feat_text += f\"{key:30s}: {float(value):.4f}\\n\"\n",
    "                        except Exception:\n",
    "                            feat_text += f\"{key:30s}: {value}\\n\"\n",
    "                \n",
    "                self.feat_text.delete('1.0', tk.END)\n",
    "                self.feat_text.insert('1.0', feat_text)\n",
    "            \n",
    "            self.status_var.set(f\"Prediction complete: {classes[pred_class]}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.status_var.set(f\"Error: {e}\")\n",
    "            messagebox.showerror(\"Error\", str(e))\n",
    "    \n",
    "    def batch_predict(self):\n",
    "        \"\"\"Batch prediction on folder\"\"\"\n",
    "        if self.model is None:\n",
    "            messagebox.showerror(\"Error\", \"No model loaded!\")\n",
    "            return\n",
    "        \n",
    "        folder_path = filedialog.askdirectory(title=\"Select folder with images\")\n",
    "        if not folder_path:\n",
    "            return\n",
    "        \n",
    "        # Implementation for batch prediction\n",
    "        messagebox.showinfo(\"Info\", \"Batch prediction feature - coming soon!\")\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN ENTRY POINT\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Try to set multiprocessing start method for safety on Windows (no forceful override)\n",
    "    try:\n",
    "        import multiprocessing as mp\n",
    "        # Only set if not already set; on some platforms setting it twice causes RuntimeError\n",
    "        # We won't force-override to avoid issues in imported contexts\n",
    "        mp.set_start_method('spawn', force=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"Professional Plantar Pressure Analysis System\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Device: {Config.DEVICE}\")\n",
    "    print(f\"Dataset: {Config.DATASET_DIR}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    Config.create_directories()\n",
    "    \n",
    "    root = tk.Tk()\n",
    "    app = ProfessionalPlantarApp(root)\n",
    "    root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

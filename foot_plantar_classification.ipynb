{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bfbf59-56d3-480a-8aa9-781d82a244a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Professional Plantar Pressure Analysis System\n",
      "============================================================\n",
      "Device: cuda\n",
      "Dataset: C:\\Users\\koust\\AnaKonda\\FOOT_PLANTAR_CLASSIFICATION\\Dataset\n",
      "============================================================\n",
      "Loaded 202 existing labels\n",
      "Extracting features from 202 images...\n",
      "Extracted features from 202 images\n",
      "Features saved to features\\extracted_features.csv\n",
      "Silhouette Score: 0.160\n",
      "Clustering results saved to results\\clustering_kmeans_results.csv\n",
      "Visualization saved to results\\clustering_visualization.png\n",
      "\n",
      "Training on cuda\n",
      "Train size: 161\n",
      "Val size: 41\n",
      "============================================================\n",
      "Epoch 1/100\n",
      "  Train Loss: 0.7161 | Train Acc: 0.4596\n",
      "  Val Loss: 0.7031 | Val Acc: 0.4634\n",
      "  LR: 0.000100\n",
      "  ✓ Model saved (Val Loss improved)\n",
      "\n",
      "Epoch 2/100\n",
      "  Train Loss: 0.6995 | Train Acc: 0.5404\n",
      "  Val Loss: 0.7065 | Val Acc: 0.4390\n",
      "  LR: 0.000100\n",
      "\n",
      "Epoch 3/100\n",
      "  Train Loss: 0.6774 | Train Acc: 0.5404\n",
      "  Val Loss: 0.6872 | Val Acc: 0.5122\n",
      "  LR: 0.000100\n",
      "  ✓ Model saved (Val Loss improved)\n",
      "\n",
      "Epoch 4/100\n",
      "  Train Loss: 0.6883 | Train Acc: 0.5714\n",
      "  Val Loss: 0.6754 | Val Acc: 0.7073\n",
      "  LR: 0.000100\n",
      "  ✓ Model saved (Val Loss improved)\n",
      "\n",
      "Epoch 5/100\n",
      "  Train Loss: 0.6895 | Train Acc: 0.5466\n",
      "  Val Loss: 0.6770 | Val Acc: 0.6341\n",
      "  LR: 0.000100\n",
      "\n",
      "Epoch 6/100\n",
      "  Train Loss: 0.6827 | Train Acc: 0.4907\n",
      "  Val Loss: 0.7192 | Val Acc: 0.4146\n",
      "  LR: 0.000100\n",
      "\n",
      "Epoch 7/100\n",
      "  Train Loss: 0.6574 | Train Acc: 0.6398\n",
      "  Val Loss: 0.7063 | Val Acc: 0.4146\n",
      "  LR: 0.000100\n",
      "\n",
      "Epoch 8/100\n",
      "  Train Loss: 0.6435 | Train Acc: 0.6770\n",
      "  Val Loss: 0.6932 | Val Acc: 0.4878\n",
      "  LR: 0.000100\n",
      "\n",
      "Epoch 9/100\n",
      "  Train Loss: 0.6409 | Train Acc: 0.6398\n",
      "  Val Loss: 0.7156 | Val Acc: 0.3902\n",
      "  LR: 0.000100\n",
      "\n",
      "Epoch 10/100\n",
      "  Train Loss: 0.6440 | Train Acc: 0.6708\n",
      "  Val Loss: 0.6719 | Val Acc: 0.6585\n",
      "  LR: 0.000100\n",
      "  ✓ Model saved (Val Loss improved)\n",
      "\n",
      "Epoch 11/100\n",
      "  Train Loss: 0.6233 | Train Acc: 0.6273\n",
      "  Val Loss: 0.7248 | Val Acc: 0.6341\n",
      "  LR: 0.000100\n",
      "\n",
      "Epoch 12/100\n",
      "  Train Loss: 0.6084 | Train Acc: 0.6770\n",
      "  Val Loss: 0.7086 | Val Acc: 0.5122\n",
      "  LR: 0.000100\n",
      "\n",
      "Epoch 13/100\n",
      "  Train Loss: 0.6197 | Train Acc: 0.6584\n",
      "  Val Loss: 0.8243 | Val Acc: 0.4634\n",
      "  LR: 0.000100\n",
      "\n",
      "Epoch 14/100\n",
      "  Train Loss: 0.5556 | Train Acc: 0.7081\n",
      "  Val Loss: 0.8563 | Val Acc: 0.5854\n",
      "  LR: 0.000100\n",
      "\n",
      "Epoch 15/100\n",
      "  Train Loss: 0.5839 | Train Acc: 0.6770\n",
      "  Val Loss: 0.8907 | Val Acc: 0.4878\n",
      "  LR: 0.000100\n",
      "\n",
      "[LR Scheduler] Reduced LR: 0.000100 -> 0.000050\n",
      "Epoch 16/100\n",
      "  Train Loss: 0.5417 | Train Acc: 0.7329\n",
      "  Val Loss: 1.0141 | Val Acc: 0.6098\n",
      "  LR: 0.000050\n",
      "\n",
      "Epoch 17/100\n",
      "  Train Loss: 0.5610 | Train Acc: 0.6708\n",
      "  Val Loss: 0.9417 | Val Acc: 0.5854\n",
      "  LR: 0.000050\n",
      "\n",
      "Epoch 18/100\n",
      "  Train Loss: 0.5096 | Train Acc: 0.7329\n",
      "  Val Loss: 0.9668 | Val Acc: 0.4878\n",
      "  LR: 0.000050\n",
      "\n",
      "Epoch 19/100\n",
      "  Train Loss: 0.4395 | Train Acc: 0.8137\n",
      "  Val Loss: 1.1041 | Val Acc: 0.4634\n",
      "  LR: 0.000050\n",
      "\n",
      "Epoch 20/100\n",
      "  Train Loss: 0.4790 | Train Acc: 0.7950\n",
      "  Val Loss: 1.1007 | Val Acc: 0.4878\n",
      "  LR: 0.000050\n",
      "\n",
      "Epoch 21/100\n",
      "  Train Loss: 0.4185 | Train Acc: 0.8509\n",
      "  Val Loss: 1.0272 | Val Acc: 0.5122\n",
      "  LR: 0.000050\n",
      "\n",
      "[LR Scheduler] Reduced LR: 0.000050 -> 0.000025\n",
      "Epoch 22/100\n",
      "  Train Loss: 0.4247 | Train Acc: 0.8323\n",
      "  Val Loss: 1.0068 | Val Acc: 0.5854\n",
      "  LR: 0.000025\n",
      "\n",
      "Epoch 23/100\n",
      "  Train Loss: 0.3868 | Train Acc: 0.8571\n",
      "  Val Loss: 1.1218 | Val Acc: 0.4146\n",
      "  LR: 0.000025\n",
      "\n",
      "Epoch 24/100\n",
      "  Train Loss: 0.4024 | Train Acc: 0.8199\n",
      "  Val Loss: 1.1409 | Val Acc: 0.5122\n",
      "  LR: 0.000025\n",
      "\n",
      "Epoch 25/100\n",
      "  Train Loss: 0.3639 | Train Acc: 0.8696\n",
      "  Val Loss: 1.1355 | Val Acc: 0.5122\n",
      "  LR: 0.000025\n",
      "\n",
      "Early stopping triggered after 25 epochs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import threading\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image, ImageTk, ImageFile\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox, ttk\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    RESAMPLE_LANCZOS = Image.Resampling.LANCZOS\n",
    "except Exception:\n",
    "    RESAMPLE_LANCZOS = Image.LANCZOS\n",
    "\n",
    "\n",
    "\n",
    "class Config:\n",
    "    \n",
    "    DATASET_DIR = r\"C:\\Users\\koust\\AnaKonda\\FOOT_PLANTAR_CLASSIFICATION\\Dataset\"\n",
    "    LABELS_CSV = \"plantar_labels.csv\"\n",
    "    MODEL_SAVE_PATH = \"models/plantar_model.pth\"\n",
    "    RESULTS_DIR = \"results\"\n",
    "    FEATURES_DIR = \"features\"\n",
    "    \n",
    "    IMAGE_SIZE = (224, 224)  \n",
    "    BATCH_SIZE = 16\n",
    "    EPOCHS = 100\n",
    "    LEARNING_RATE = 0.0001\n",
    "    EARLY_STOPPING_PATIENCE = 15\n",
    "    \n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    FEATURE_METHODS = ['pressure_stats', 'pressure_distribution', 'asymmetry', 'cog']\n",
    "    \n",
    "    N_CLUSTERS = 3  \n",
    "    \n",
    "    @classmethod\n",
    "    def create_directories(cls):\n",
    "        for dir_path in [cls.RESULTS_DIR, cls.FEATURES_DIR, 'models', 'logs']:\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "class PressureMapFeatureExtractor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_names = []\n",
    "    \n",
    "    def extract_all_features(self, image_path):\n",
    "        try:\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Could not load image: {image_path}\")\n",
    "            \n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            features = {}\n",
    "            features.update(self._extract_pressure_statistics(gray))\n",
    "            features.update(self._extract_pressure_distribution(gray))\n",
    "            features.update(self._extract_spatial_features(gray))\n",
    "            features.update(self._extract_asymmetry_features(image))\n",
    "            \n",
    "            return features\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting features from {image_path}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "    \n",
    "    def _extract_pressure_statistics(self, gray_image):\n",
    "        features = {\n",
    "            'mean_pressure': float(np.mean(gray_image)),\n",
    "            'std_pressure': float(np.std(gray_image)),\n",
    "            'max_pressure': float(np.max(gray_image)),\n",
    "            'min_pressure': float(np.min(gray_image)),\n",
    "            'median_pressure': float(np.median(gray_image)),\n",
    "            'pressure_range': float(np.max(gray_image) - np.min(gray_image)),\n",
    "        }\n",
    "        return features\n",
    "    \n",
    "    def _extract_pressure_distribution(self, gray_image):\n",
    "        hist, _ = np.histogram(gray_image, bins=10, range=(0, 255))\n",
    "        hist = hist.astype(float)\n",
    "        if hist.sum() > 0:\n",
    "            hist = hist / hist.sum()  \n",
    "        else:\n",
    "            hist = np.zeros_like(hist, dtype=float)\n",
    "        \n",
    "        features = {f'pressure_bin_{i}': float(val) for i, val in enumerate(hist)}\n",
    "        \n",
    "        high_pressure_threshold = np.percentile(gray_image, 75)\n",
    "        high_pressure_area = float(np.sum(gray_image > high_pressure_threshold) / gray_image.size)\n",
    "        features['high_pressure_area_ratio'] = high_pressure_area\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _extract_spatial_features(self, gray_image):\n",
    "        y_coords, x_coords = np.indices(gray_image.shape)\n",
    "        total_pressure = float(np.sum(gray_image))\n",
    "        \n",
    "        if total_pressure > 0:\n",
    "            cog_x = float(np.sum(x_coords * gray_image) / total_pressure)\n",
    "            cog_y = float(np.sum(y_coords * gray_image) / total_pressure)\n",
    "        else:\n",
    "            cog_x, cog_y = float(gray_image.shape[1] / 2), float(gray_image.shape[0] / 2)\n",
    "        \n",
    "        features = {\n",
    "            'cog_x_normalized': float(cog_x / gray_image.shape[1]),\n",
    "            'cog_y_normalized': float(cog_y / gray_image.shape[0]),\n",
    "        }\n",
    "        \n",
    "        contact_threshold = float(np.mean(gray_image) + np.std(gray_image))\n",
    "        contact_area = float(np.sum(gray_image > contact_threshold) / gray_image.size)\n",
    "        features['contact_area_ratio'] = contact_area\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _extract_asymmetry_features(self, image):\n",
    "        h, w = image.shape[:2]\n",
    "        left_half = image[:, :w//2]\n",
    "        right_half = image[:, w//2:]\n",
    "        \n",
    "        right_flipped = cv2.flip(right_half, 1)\n",
    "        \n",
    "        min_width = min(left_half.shape[1], right_flipped.shape[1])\n",
    "        left_half = left_half[:, :min_width]\n",
    "        right_flipped = right_flipped[:, :min_width]\n",
    "        \n",
    "        diff = cv2.absdiff(left_half, right_flipped)\n",
    "        asymmetry_score = float(np.mean(diff) / 255.0) if diff.size else 0.0\n",
    "        \n",
    "        features = {\n",
    "            'lr_asymmetry': asymmetry_score,\n",
    "            'left_mean_pressure': float(np.mean(left_half)) if left_half.size else 0.0,\n",
    "            'right_mean_pressure': float(np.mean(right_half)) if right_half.size else 0.0,\n",
    "        }\n",
    "        \n",
    "        return features\n",
    "\n",
    "\n",
    "\n",
    "class ImprovedPlantarCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=2, pretrained=True):\n",
    "        super(ImprovedPlantarCNN, self).__init__()\n",
    "        \n",
    "        self.backbone = models.resnet18(pretrained=pretrained)\n",
    "        \n",
    "        self.backbone.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, \n",
    "                                        padding=3, bias=False)\n",
    "        \n",
    "        num_features = self.backbone.fc.in_features\n",
    "        \n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "\n",
    "class PlantarDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_file, transform=None, augment=False):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        self.label_map = {'healthy': 0, 'unhealthy': 1, 'normal': 0, 'abnormal': 1}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx]['image_path']\n",
    "        label_str = self.df.iloc[idx]['label']\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"[Dataset] Error loading image at idx={idx}, path={img_path}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            image = Image.new('RGB', Config.IMAGE_SIZE, color='black')\n",
    "            label_str = 'healthy'\n",
    "        \n",
    "        label = self.label_map.get(str(label_str).lower(), 0)\n",
    "        \n",
    "        if self.transform:\n",
    "            try:\n",
    "                image = self.transform(image)\n",
    "            except Exception as e:\n",
    "                print(f\"[Dataset] Transform failed for idx={idx}, path={img_path}: {e}\")\n",
    "                traceback.print_exc()\n",
    "                # Default fallback tensor\n",
    "                image = transforms.ToTensor()(Image.new('RGB', Config.IMAGE_SIZE, color='black'))\n",
    "        \n",
    "        return image, label, img_path\n",
    "\n",
    "\n",
    "class TrainingEngine:\n",
    "    \n",
    "    def __init__(self, model, train_loader, val_loader, device):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        \n",
    "        self.optimizer = optim.AdamW(\n",
    "            self.model.parameters(), \n",
    "            lr=Config.LEARNING_RATE,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        try:\n",
    "            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "            )\n",
    "            self._scheduler_supports_verbose = True\n",
    "        except TypeError:\n",
    "            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer, mode='min', factor=0.5, patience=5\n",
    "            )\n",
    "            self._scheduler_supports_verbose = False\n",
    "        \n",
    "        self._last_lr = float(self.optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        self.best_val_loss = float('inf')\n",
    "        self.patience_counter = 0\n",
    "        \n",
    "        self.history = {\n",
    "            'train_loss': [], 'val_loss': [], \n",
    "            'train_acc': [], 'val_acc': [],\n",
    "            'learning_rates': []\n",
    "        }\n",
    "\n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels, _ in self.train_loader:\n",
    "            images, labels = images.to(self.device), labels.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(self.train_loader.dataset) if len(self.train_loader.dataset)>0 else 0.0\n",
    "        epoch_acc = correct / total if total > 0 else 0.0\n",
    "        \n",
    "        return epoch_loss, epoch_acc\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels, _ in self.val_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_loss = running_loss / len(self.val_loader.dataset) if len(self.val_loader.dataset)>0 else float('inf')\n",
    "        val_acc = correct / total if total>0 else 0.0\n",
    "        \n",
    "        return val_loss, val_acc, all_preds, all_labels\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        print(f\"\\nTraining on {self.device}\")\n",
    "        print(f\"Train size: {len(self.train_loader.dataset)}\")\n",
    "        print(f\"Val size: {len(self.val_loader.dataset)}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss, train_acc = self.train_epoch()\n",
    "            val_loss, val_acc, _, _ = self.validate()\n",
    "            \n",
    "            self.scheduler.step(val_loss)\n",
    "            current_lr = float(self.optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "            if not self._scheduler_supports_verbose and current_lr < self._last_lr:\n",
    "                print(f\"[LR Scheduler] Reduced LR: {self._last_lr:.6f} -> {current_lr:.6f}\")\n",
    "            self._last_lr = current_lr\n",
    "            \n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_acc'].append(val_acc)\n",
    "            self.history['learning_rates'].append(current_lr)\n",
    "            \n",
    "            print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "            print(f'  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}')\n",
    "            print(f'  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n",
    "            print(f'  LR: {current_lr:.6f}')\n",
    "            \n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.patience_counter = 0\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_acc': val_acc,\n",
    "                }, Config.MODEL_SAVE_PATH)\n",
    "                print(f'  ✓ Model saved (Val Loss improved)')\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                if self.patience_counter >= Config.EARLY_STOPPING_PATIENCE:\n",
    "                    print(f'\\nEarly stopping triggered after {epoch+1} epochs')\n",
    "                    break\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        return self.history\n",
    "\n",
    "\n",
    "\n",
    "class UnsupervisedAnalyzer:\n",
    "    \n",
    "    def __init__(self, image_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.feature_extractor = PressureMapFeatureExtractor()\n",
    "        self.features_df = None\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def extract_features_from_dataset(self):\n",
    "        image_files = [f for f in os.listdir(self.image_dir) \n",
    "                      if f.lower().endswith(('.jpg', '.jpeg', '.png', '.tiff', '.bmp'))]\n",
    "        \n",
    "        features_list = []\n",
    "        valid_files = []\n",
    "        \n",
    "        print(f\"Extracting features from {len(image_files)} images...\")\n",
    "        for img_file in image_files:\n",
    "            img_path = os.path.join(self.image_dir, img_file)\n",
    "            features = self.feature_extractor.extract_all_features(img_path)\n",
    "            if features is not None:\n",
    "                features['filename'] = img_file\n",
    "                features_list.append(features)\n",
    "                valid_files.append(img_file)\n",
    "        \n",
    "        self.features_df = pd.DataFrame(features_list)\n",
    "        print(f\"Extracted features from {len(features_list)} images\")\n",
    "        \n",
    "        features_path = os.path.join(Config.FEATURES_DIR, 'extracted_features.csv')\n",
    "        try:\n",
    "            self.features_df.to_csv(features_path, index=False, encoding='utf-8-sig')\n",
    "        except Exception:\n",
    "            self.features_df.to_csv(features_path, index=False)\n",
    "        print(f\"Features saved to {features_path}\")\n",
    "        \n",
    "        return self.features_df\n",
    "    \n",
    "    def perform_clustering(self, n_clusters=3, method='kmeans'):\n",
    "        if self.features_df is None:\n",
    "            print(\"No features available. Extract features first.\")\n",
    "            return None\n",
    "        \n",
    "        feature_cols = [col for col in self.features_df.columns if col != 'filename']\n",
    "        X = self.features_df[feature_cols].values\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        if method == 'kmeans':\n",
    "            clusterer = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        elif method == 'dbscan':\n",
    "            clusterer = DBSCAN(eps=0.5, min_samples=5)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "        \n",
    "        labels = clusterer.fit_predict(X_scaled)\n",
    "        \n",
    "        self.features_df['cluster'] = labels\n",
    "        \n",
    "        if len(set(labels)) > 1:\n",
    "            try:\n",
    "                sil_score = silhouette_score(X_scaled, labels)\n",
    "                print(f\"Silhouette Score: {sil_score:.3f}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        results_path = os.path.join(Config.RESULTS_DIR, f'clustering_{method}_results.csv')\n",
    "        try:\n",
    "            self.features_df.to_csv(results_path, index=False, encoding='utf-8-sig')\n",
    "        except Exception:\n",
    "            self.features_df.to_csv(results_path, index=False)\n",
    "        print(f\"Clustering results saved to {results_path}\")\n",
    "        \n",
    "        return labels, X_scaled\n",
    "    \n",
    "    def visualize_clusters(self, X_scaled, labels):\n",
    "        pca = PCA(n_components=2)\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, \n",
    "                            cmap='viridis', alpha=0.6, edgecolors='k')\n",
    "        plt.colorbar(scatter, label='Cluster')\n",
    "        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "        plt.title('Cluster Visualization (PCA)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        plt.bar(unique, counts, color='skyblue', edgecolor='black')\n",
    "        plt.xlabel('Cluster')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Cluster Distribution')\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_path = os.path.join(Config.RESULTS_DIR, 'clustering_visualization.png')\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Visualization saved to {plot_path}\")\n",
    "        \n",
    "        return X_pca\n",
    "\n",
    "\n",
    "class GradCAM:\n",
    "    \n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        target_layer.register_forward_hook(self.save_activation)\n",
    "        target_layer.register_backward_hook(self.save_gradient)\n",
    "    \n",
    "    def save_activation(self, module, input, output):\n",
    "        self.activations = output.detach()\n",
    "    \n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0].detach()\n",
    "    \n",
    "    def generate_cam(self, input_image, target_class=None):\n",
    "        self.model.eval()\n",
    "        \n",
    "        output = self.model(input_image)\n",
    "        \n",
    "        if target_class is None:\n",
    "            target_class = output.argmax(dim=1).item()\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        target = output[0, target_class]\n",
    "        target.backward()\n",
    "        \n",
    "        weights = self.gradients.mean(dim=(2, 3), keepdim=True)\n",
    "        \n",
    "        cam = (weights * self.activations).sum(dim=1, keepdim=True)\n",
    "        cam = torch.relu(cam)\n",
    "        \n",
    "        cam = cam - cam.min()\n",
    "        cam = cam / (cam.max() + 1e-8)\n",
    "        \n",
    "        return cam.squeeze().cpu().numpy(), target_class\n",
    "\n",
    "\n",
    "class LabelingTool:\n",
    "    \n",
    "    def __init__(self, root, image_dir):\n",
    "        self.root = root\n",
    "        self.image_dir = image_dir\n",
    "        self.current_idx = 0\n",
    "        self.labels = {}\n",
    "        self.current_image = None   \n",
    "        self.current_photo = None  \n",
    "        \n",
    "        self.image_files = [f for f in os.listdir(image_dir)\n",
    "                          if f.lower().endswith(('.jpg', '.jpeg', '.png', '.tiff', '.bmp'))]\n",
    "        self.image_files.sort()\n",
    "        \n",
    "        if os.path.exists(Config.LABELS_CSV):\n",
    "            try:\n",
    "                df = pd.read_csv(Config.LABELS_CSV)\n",
    "                self.labels = dict(zip(df['image_path'], df['label']))\n",
    "                print(f\"Loaded {len(self.labels)} existing labels\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        self.create_widgets()\n",
    "        self.root.after(100, lambda: self.load_image(self.current_idx))\n",
    "    \n",
    "    def create_widgets(self):\n",
    "        self.root.title(\"Plantar Pressure Image Labeling Tool\")\n",
    "        self.root.geometry(\"900x700\")\n",
    "        \n",
    "        info_frame = ttk.Frame(self.root)\n",
    "        info_frame.pack(fill=tk.X, padx=10, pady=5)\n",
    "        \n",
    "        self.progress_label = ttk.Label(info_frame, \n",
    "                                       text=f\"Image 0/{len(self.image_files)}\", \n",
    "                                       font=('Arial', 12, 'bold'))\n",
    "        self.progress_label.pack(side=tk.LEFT)\n",
    "        \n",
    "        labeled_count = len([v for v in self.labels.values() if v])\n",
    "        self.labeled_label = ttk.Label(info_frame, \n",
    "                                       text=f\"Labeled: {labeled_count}\", \n",
    "                                       font=('Arial', 12))\n",
    "        self.labeled_label.pack(side=tk.RIGHT)\n",
    "        \n",
    "        \n",
    "        img_frame = ttk.LabelFrame(self.root, text=\"Plantar Pressure Map\")\n",
    "        img_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)\n",
    "        \n",
    "        self.canvas = tk.Canvas(img_frame, bg='black', highlightthickness=1, highlightbackground='#cccccc')\n",
    "        self.canvas.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        self.canvas.bind(\"<Configure>\", self._on_canvas_configure)\n",
    "        \n",
    "        self.filename_label = ttk.Label(self.root, text=\"\", \n",
    "                                       font=('Arial', 10))\n",
    "        self.filename_label.pack(pady=2)\n",
    "        \n",
    "        button_frame = ttk.LabelFrame(self.root, text=\"Classification\")\n",
    "        button_frame.pack(fill=tk.X, padx=10, pady=5)\n",
    "        \n",
    "        btn_frame_inner = ttk.Frame(button_frame)\n",
    "        btn_frame_inner.pack(pady=10)\n",
    "        \n",
    "        ttk.Button(btn_frame_inner, text=\"Normal\", command=lambda: self.label_current('healthy')).pack(side=tk.LEFT, padx=10)\n",
    "        ttk.Button(btn_frame_inner, text=\"Abnormal\", command=lambda: self.label_current('unhealthy')).pack(side=tk.LEFT, padx=10)\n",
    "        ttk.Button(btn_frame_inner, text=\"Skip\", command=self.skip_image).pack(side=tk.LEFT, padx=10)\n",
    "        \n",
    "        nav_frame = ttk.Frame(self.root)\n",
    "        nav_frame.pack(fill=tk.X, padx=10, pady=5)\n",
    "        \n",
    "        ttk.Button(nav_frame, text=\"◄ Previous\", command=self.prev_image).pack(side=tk.LEFT, padx=5)\n",
    "        ttk.Button(nav_frame, text=\"Next ►\", command=self.next_image).pack(side=tk.LEFT, padx=5)\n",
    "        ttk.Button(nav_frame, text=\"Save & Exit\", command=self.save_and_exit).pack(side=tk.RIGHT, padx=5)\n",
    "        \n",
    "        self.root.bind('1', lambda e: self.label_current('healthy'))\n",
    "        self.root.bind('2', lambda e: self.label_current('unhealthy'))\n",
    "        self.root.bind('<space>', lambda e: self.skip_image())\n",
    "        self.root.bind('<Left>', lambda e: self.prev_image())\n",
    "        self.root.bind('<Right>', lambda e: self.next_image())\n",
    "    \n",
    "    def _on_canvas_configure(self, event):\n",
    "        if self.current_image is not None:\n",
    "            self._display_image_on_canvas(self.current_image)\n",
    "    \n",
    "    def load_image(self, idx):\n",
    "        if idx < 0 or idx >= len(self.image_files):\n",
    "            return\n",
    "        \n",
    "        self.current_idx = idx\n",
    "        filename = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, filename)\n",
    "        \n",
    "        try:\n",
    "            pil_image = Image.open(img_path).convert('RGB')\n",
    "            self.current_image = pil_image  \n",
    "            \n",
    "            self._display_image_on_canvas(pil_image)\n",
    "            \n",
    "            self.progress_label.config(text=f\"Image {idx+1}/{len(self.image_files)}\")\n",
    "            \n",
    "            current_label = self.labels.get(img_path, \"Not labeled\")\n",
    "            self.filename_label.config(text=f\"{filename} | Current: {current_label}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Failed to load image: {e}\")\n",
    "            self.current_image = None\n",
    "            self.canvas.delete(\"all\")\n",
    "    \n",
    "    def _display_image_on_canvas(self, pil_image):\n",
    "        canvas_width = max(1, self.canvas.winfo_width())\n",
    "        canvas_height = max(1, self.canvas.winfo_height())\n",
    "        \n",
    "        if canvas_width < 10 or canvas_height < 10:\n",
    "            self.root.after(100, lambda: self._display_image_on_canvas(pil_image))\n",
    "            return\n",
    "        \n",
    "        max_w = max(1, canvas_width - 10)\n",
    "        max_h = max(1, canvas_height - 10)\n",
    "        \n",
    "        image_copy = pil_image.copy()\n",
    "        image_copy.thumbnail((max_w, max_h), RESAMPLE_LANCZOS)\n",
    "        \n",
    "        self.current_photo = ImageTk.PhotoImage(image_copy)\n",
    "        \n",
    "        self.canvas.delete(\"all\")\n",
    "        self.canvas.create_image(0, 0, image=self.current_photo, anchor='nw')\n",
    "    \n",
    "    def label_current(self, label):\n",
    "        filename = self.image_files[self.current_idx]\n",
    "        img_path = os.path.join(self.image_dir, filename)\n",
    "        self.labels[img_path] = label\n",
    "        \n",
    "        labeled_count = len([v for v in self.labels.values() if v])\n",
    "        self.labeled_label.config(text=f\"Labeled: {labeled_count}\")\n",
    "        \n",
    "        self.next_image()\n",
    "    \n",
    "    def skip_image(self):\n",
    "        self.next_image()\n",
    "    \n",
    "    def next_image(self):\n",
    "        if self.current_idx < len(self.image_files) - 1:\n",
    "            self.load_image(self.current_idx + 1)\n",
    "    \n",
    "    def prev_image(self):\n",
    "        if self.current_idx > 0:\n",
    "            self.load_image(self.current_idx - 1)\n",
    "    \n",
    "    def save_and_exit(self):\n",
    "        if not self.labels:\n",
    "            messagebox.showwarning(\"Warning\", \"No labels to save!\")\n",
    "            return\n",
    "        \n",
    "        data = []\n",
    "        for img_path, label in self.labels.items():\n",
    "            data.append({'image_path': img_path, 'label': label})\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        try:\n",
    "            df.to_csv(Config.LABELS_CSV, index=False, encoding='utf-8-sig')\n",
    "        except Exception:\n",
    "            df.to_csv(Config.LABELS_CSV, index=False)\n",
    "        \n",
    "        messagebox.showinfo(\"Success\", f\"Saved {len(self.labels)} labels to {Config.LABELS_CSV}\")\n",
    "        self.root.destroy()\n",
    "\n",
    "\n",
    "class ProfessionalPlantarApp:\n",
    "    \n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Professional Plantar Pressure Analysis System\")\n",
    "        self.root.geometry(\"1200x800\")\n",
    "        \n",
    "        Config.create_directories()\n",
    "        \n",
    "        self.model = None\n",
    "        self.unsupervised_analyzer = UnsupervisedAnalyzer(Config.DATASET_DIR)\n",
    "        self.feature_extractor = PressureMapFeatureExtractor()\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(Config.IMAGE_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.create_gui()\n",
    "        self.load_model_if_exists()\n",
    "    \n",
    "    def create_gui(self):\n",
    "        menubar = tk.Menu(self.root)\n",
    "        self.root.config(menu=menubar)\n",
    "        \n",
    "        file_menu = tk.Menu(menubar, tearoff=0)\n",
    "        menubar.add_cascade(label=\"File\", menu=file_menu)\n",
    "        file_menu.add_command(label=\"Label Images\", command=self.open_labeling_tool)\n",
    "        file_menu.add_separator()\n",
    "        file_menu.add_command(label=\"Exit\", command=self.root.quit)\n",
    "        \n",
    "        analysis_menu = tk.Menu(menubar, tearoff=0)\n",
    "        menubar.add_cascade(label=\"Analysis\", menu=analysis_menu)\n",
    "        analysis_menu.add_command(label=\"Extract Features\", \n",
    "                                 command=self.extract_features)\n",
    "        analysis_menu.add_command(label=\"Cluster Analysis\", \n",
    "                                 command=self.run_clustering)\n",
    "        analysis_menu.add_command(label=\"View Results\", \n",
    "                                 command=self.view_results)\n",
    "        \n",
    "        model_menu = tk.Menu(menubar, tearoff=0)\n",
    "        menubar.add_cascade(label=\"Model\", menu=model_menu)\n",
    "        model_menu.add_command(label=\"Train Model\", command=self.train_model)\n",
    "        model_menu.add_command(label=\"Load Model\", command=self.load_model_if_exists)\n",
    "        \n",
    "        self.notebook = ttk.Notebook(self.root)\n",
    "        self.notebook.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "        \n",
    "        self.create_prediction_tab()\n",
    "        \n",
    "        self.create_analysis_tab()\n",
    "        \n",
    "        self.status_var = tk.StringVar(value=\"Ready\")\n",
    "        status_bar = ttk.Label(self.root, textvariable=self.status_var, \n",
    "                              relief=tk.SUNKEN, anchor=tk.W)\n",
    "        status_bar.pack(side=tk.BOTTOM, fill=tk.X)\n",
    "    \n",
    "    def create_prediction_tab(self):\n",
    "        pred_frame = ttk.Frame(self.notebook)\n",
    "        self.notebook.add(pred_frame, text=\"Image Prediction\")\n",
    "        \n",
    "        control_frame = ttk.LabelFrame(pred_frame, text=\"Controls\")\n",
    "        control_frame.pack(fill=tk.X, padx=10, pady=5)\n",
    "        \n",
    "        ttk.Button(control_frame, text=\"Load Image\", \n",
    "                  command=self.load_and_predict).pack(side=tk.LEFT, padx=5, pady=5)\n",
    "        ttk.Button(control_frame, text=\"Batch Predict\", \n",
    "                  command=self.batch_predict).pack(side=tk.LEFT, padx=5, pady=5)\n",
    "        \n",
    "        content_frame = ttk.Frame(pred_frame)\n",
    "        content_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)\n",
    "        \n",
    "        left_frame = ttk.LabelFrame(content_frame, text=\"Image\")\n",
    "        left_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=5)\n",
    "        \n",
    "        self.image_canvas = tk.Canvas(left_frame, bg='black')\n",
    "        self.image_canvas.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        self.image_canvas.bind(\"<Configure>\", lambda e: None)  # keep to allow future resizing behavior\n",
    "        \n",
    "        right_frame = ttk.LabelFrame(content_frame, text=\"Results\")\n",
    "        right_frame.pack(side=tk.RIGHT, fill=tk.BOTH, expand=True, padx=5)\n",
    "        \n",
    "        result_frame = ttk.Frame(right_frame)\n",
    "        result_frame.pack(fill=tk.X, pady=10, padx=10)\n",
    "        \n",
    "        ttk.Label(result_frame, text=\"Classification:\", \n",
    "                 font=('Arial', 11, 'bold')).pack(anchor=tk.W)\n",
    "        self.pred_result_var = tk.StringVar(value=\"No prediction\")\n",
    "        self.pred_result_label = ttk.Label(result_frame, \n",
    "                                          textvariable=self.pred_result_var,\n",
    "                                          font=('Arial', 14, 'bold'))\n",
    "        self.pred_result_label.pack(anchor=tk.W, pady=5)\n",
    "        \n",
    "        conf_frame = ttk.LabelFrame(right_frame, text=\"Confidence\")\n",
    "        conf_frame.pack(fill=tk.X, pady=10, padx=10)\n",
    "        \n",
    "        self.conf_text = tk.Text(conf_frame, height=3, font=('Consolas', 10))\n",
    "        self.conf_text.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        feat_frame = ttk.LabelFrame(right_frame, text=\"Extracted Features\")\n",
    "        feat_frame.pack(fill=tk.BOTH, expand=True, pady=10, padx=10)\n",
    "        \n",
    "        self.feat_text = tk.Text(feat_frame, font=('Consolas', 9))\n",
    "        feat_scrollbar = ttk.Scrollbar(feat_frame, command=self.feat_text.yview)\n",
    "        self.feat_text.configure(yscrollcommand=feat_scrollbar.set)\n",
    "        feat_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "        self.feat_text.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "    \n",
    "    def create_analysis_tab(self):\n",
    "        analysis_frame = ttk.Frame(self.notebook)\n",
    "        self.notebook.add(analysis_frame, text=\"Dataset Analysis\")\n",
    "        \n",
    "        self.analysis_text = tk.Text(analysis_frame, font=('Consolas', 10))\n",
    "        scrollbar = ttk.Scrollbar(analysis_frame, command=self.analysis_text.yview)\n",
    "        self.analysis_text.configure(yscrollcommand=scrollbar.set)\n",
    "        \n",
    "        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "        self.analysis_text.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "    \n",
    "    def open_labeling_tool(self):\n",
    "        labeling_window = tk.Toplevel(self.root)\n",
    "        LabelingTool(labeling_window, Config.DATASET_DIR)\n",
    "    \n",
    "    def extract_features(self):\n",
    "        self.status_var.set(\"Extracting features...\")\n",
    "        self.root.update()\n",
    "        \n",
    "        def extract_thread():\n",
    "            try:\n",
    "                features_df = self.unsupervised_analyzer.extract_features_from_dataset()\n",
    "                \n",
    "                summary = f\"Feature Extraction Complete\\n\"\n",
    "                summary += f\"{'='*50}\\n\\n\"\n",
    "                summary += f\"Total images processed: {len(features_df)}\\n\"\n",
    "                summary += f\"Total features: {len(features_df.columns)-1}\\n\\n\"\n",
    "                summary += f\"Feature Statistics:\\n\"\n",
    "                summary += f\"{'-'*50}\\n\"\n",
    "                summary += features_df.describe().to_string()\n",
    "                \n",
    "                self.analysis_text.delete('1.0', tk.END)\n",
    "                self.analysis_text.insert('1.0', summary)\n",
    "                self.status_var.set(\"Feature extraction complete\")\n",
    "            except Exception as e:\n",
    "                self.status_var.set(f\"Error: {e}\")\n",
    "                messagebox.showerror(\"Error\", str(e))\n",
    "        \n",
    "        threading.Thread(target=extract_thread, daemon=True).start()\n",
    "    \n",
    "    def run_clustering(self):\n",
    "        self.status_var.set(\"Running clustering...\")\n",
    "        self.root.update()\n",
    "        \n",
    "        def cluster_thread():\n",
    "            try:\n",
    "                labels, X_scaled = self.unsupervised_analyzer.perform_clustering(\n",
    "                    n_clusters=Config.N_CLUSTERS\n",
    "                )\n",
    "                \n",
    "                X_pca = self.unsupervised_analyzer.visualize_clusters(X_scaled, labels)\n",
    "                \n",
    "                summary = f\"Clustering Analysis Complete\\n\"\n",
    "                summary += f\"{'='*50}\\n\\n\"\n",
    "                summary += f\"Method: K-Means\\n\"\n",
    "                summary += f\"Number of clusters: {Config.N_CLUSTERS}\\n\\n\"\n",
    "                summary += f\"Cluster Distribution:\\n\"\n",
    "                summary += f\"{'-'*50}\\n\"\n",
    "                \n",
    "                for cluster_id in range(Config.N_CLUSTERS):\n",
    "                    count = np.sum(labels == cluster_id)\n",
    "                    percentage = (count / len(labels)) * 100\n",
    "                    summary += f\"Cluster {cluster_id}: {count} images ({percentage:.1f}%)\\n\"\n",
    "                \n",
    "                summary += f\"\\n\\nInterpretation:\\n\"\n",
    "                summary += f\"{'-'*50}\\n\"\n",
    "                summary += f\"Images have been grouped into {Config.N_CLUSTERS} distinct patterns.\\n\"\n",
    "                summary += f\"Review 'clustering_kmeans_results.csv' to see which images\\n\"\n",
    "                summary += f\"belong to each cluster. This can help identify:\\n\"\n",
    "                summary += f\"  - Normal vs abnormal patterns\\n\"\n",
    "                summary += f\"  - Different types of gait abnormalities\\n\"\n",
    "                summary += f\"  - Outliers or unusual cases\\n\"\n",
    "                \n",
    "                self.analysis_text.delete('1.0', tk.END)\n",
    "                self.analysis_text.insert('1.0', summary)\n",
    "                self.status_var.set(\"Clustering complete - check results folder\")\n",
    "                \n",
    "                messagebox.showinfo(\"Success\", \n",
    "                                  \"Clustering complete! Check 'results' folder for visualizations.\")\n",
    "            except Exception as e:\n",
    "                self.status_var.set(f\"Error: {e}\")\n",
    "                messagebox.showerror(\"Error\", str(e))\n",
    "        \n",
    "        threading.Thread(target=cluster_thread, daemon=True).start()\n",
    "    \n",
    "    def view_results(self):\n",
    "        results_dir = Config.RESULTS_DIR\n",
    "        if os.path.exists(results_dir):\n",
    "            try:\n",
    "                os.startfile(results_dir)  # Windows\n",
    "            except Exception:\n",
    "                messagebox.showinfo(\"Info\", f\"Results path: {os.path.abspath(results_dir)}\")\n",
    "        else:\n",
    "            messagebox.showinfo(\"Info\", \"No results available yet\")\n",
    "    \n",
    "    def load_model_if_exists(self):\n",
    "        if os.path.exists(Config.MODEL_SAVE_PATH):\n",
    "            try:\n",
    "                self.model = ImprovedPlantarCNN().to(Config.DEVICE)\n",
    "                checkpoint = torch.load(Config.MODEL_SAVE_PATH, map_location=Config.DEVICE)\n",
    "                self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                self.model.eval()\n",
    "                self.status_var.set(f\"Model loaded successfully (Val Acc: {checkpoint.get('val_acc', 0):.2%})\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                self.status_var.set(f\"Error loading model: {e}\")\n",
    "                return False\n",
    "        else:\n",
    "            self.status_var.set(\"No trained model found\")\n",
    "            return False\n",
    "    \n",
    "    def train_model(self):\n",
    "        if not os.path.exists(Config.LABELS_CSV):\n",
    "            messagebox.showerror(\"Error\", \n",
    "                               \"No labels file found! Please label images first.\")\n",
    "            return\n",
    "        \n",
    "        response = messagebox.askyesno(\"Train Model\", \n",
    "                                      \"This will start training. Continue?\")\n",
    "        if not response:\n",
    "            return\n",
    "        \n",
    "        self.status_var.set(\"Training started...\")\n",
    "        \n",
    "        def train_thread():\n",
    "            try:\n",
    "                train_transform = transforms.Compose([\n",
    "                    transforms.Resize(Config.IMAGE_SIZE),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomRotation(15),\n",
    "                    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                       std=[0.229, 0.224, 0.225])\n",
    "                ])\n",
    "                \n",
    "                val_transform = transforms.Compose([\n",
    "                    transforms.Resize(Config.IMAGE_SIZE),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                       std=[0.229, 0.224, 0.225])\n",
    "                ])\n",
    "                \n",
    "                df = pd.read_csv(Config.LABELS_CSV)\n",
    "                total_len = len(df)\n",
    "                if total_len < 2:\n",
    "                    messagebox.showerror(\"Error\", \"Not enough labeled examples to train.\")\n",
    "                    self.status_var.set(\"Training aborted: insufficient data\")\n",
    "                    return\n",
    "                \n",
    "                train_size = int(0.8 * total_len)\n",
    "                indices = list(range(total_len))\n",
    "                np.random.seed(42)\n",
    "                np.random.shuffle(indices)\n",
    "                train_indices = indices[:train_size]\n",
    "                val_indices = indices[train_size:]\n",
    "                \n",
    "                train_dataset = Subset(PlantarDataset(Config.LABELS_CSV, transform=train_transform), train_indices)\n",
    "                val_dataset = Subset(PlantarDataset(Config.LABELS_CSV, transform=val_transform), val_indices)\n",
    "                \n",
    "                train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, \n",
    "                                        shuffle=True, num_workers=0, pin_memory=False)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE, \n",
    "                                      num_workers=0, pin_memory=False)\n",
    "                \n",
    "                model = ImprovedPlantarCNN(pretrained=True)\n",
    "                \n",
    "                engine = TrainingEngine(model, train_loader, val_loader, Config.DEVICE)\n",
    "                history = engine.train(Config.EPOCHS)\n",
    "                \n",
    "                self.status_var.set(\"Training complete!\")\n",
    "                messagebox.showinfo(\"Success\", \"Model training complete!\")\n",
    "                \n",
    "                self.load_model_if_exists()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(\"Training error:\", e)\n",
    "                traceback.print_exc()\n",
    "                self.status_var.set(f\"Training error: {e}\")\n",
    "                messagebox.showerror(\"Error\", str(e))\n",
    "        \n",
    "        threading.Thread(target=train_thread, daemon=True).start()\n",
    "    \n",
    "    def load_and_predict(self):\n",
    "        \"\"\"Load image and make prediction\"\"\"\n",
    "        if self.model is None:\n",
    "            messagebox.showerror(\"Error\", \"No model loaded!\")\n",
    "            return\n",
    "        \n",
    "        file_path = filedialog.askopenfilename(\n",
    "            filetypes=[(\"Image files\", \"*.jpg;*.jpeg;*.png;*.tiff;*.bmp\")]\n",
    "        )\n",
    "        if not file_path:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(file_path).convert('RGB')\n",
    "            display_img = image.copy()\n",
    "            canvas_width = max(1, self.image_canvas.winfo_width())\n",
    "            canvas_height = max(1, self.image_canvas.winfo_height())\n",
    "            if canvas_width > 10 and canvas_height > 10:\n",
    "                display_img.thumbnail((canvas_width-10, canvas_height-10), RESAMPLE_LANCZOS)\n",
    "            \n",
    "            photo = ImageTk.PhotoImage(display_img)\n",
    "            \n",
    "            self.image_canvas.delete(\"all\")\n",
    "            self.image_canvas.image = photo\n",
    "            self.image_canvas.create_image(0, 0, image=photo, anchor='nw')\n",
    "            \n",
    "            input_tensor = self.transform(image).unsqueeze(0).to(Config.DEVICE)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_tensor)\n",
    "                probabilities = torch.softmax(outputs, dim=1)[0]\n",
    "            \n",
    "            classes = ['Normal (Healthy)', 'Abnormal (Unhealthy)']\n",
    "            pred_class = int(probabilities.argmax().item())\n",
    "            confidence = float(probabilities[pred_class].item() * 100)\n",
    "            \n",
    "            self.pred_result_var.set(classes[pred_class])\n",
    "            color = \"green\" if pred_class == 0 else \"red\"\n",
    "            self.pred_result_label.config(foreground=color)\n",
    "            \n",
    "            conf_text = f\"Normal:   {probabilities[0].item()*100:.2f}%\\n\"\n",
    "            conf_text += f\"Abnormal: {probabilities[1].item()*100:.2f}%\\n\"\n",
    "            conf_text += f\"Confidence: {confidence:.2f}%\"\n",
    "            \n",
    "            self.conf_text.delete('1.0', tk.END)\n",
    "            self.conf_text.insert('1.0', conf_text)\n",
    "            \n",
    "            features = self.feature_extractor.extract_all_features(file_path)\n",
    "            if features:\n",
    "                feat_text = \"Key Features:\\n\" + \"=\"*40 + \"\\n\\n\"\n",
    "                for key, value in sorted(features.items()):\n",
    "                    if key != 'filename':\n",
    "                        try:\n",
    "                            feat_text += f\"{key:30s}: {float(value):.4f}\\n\"\n",
    "                        except Exception:\n",
    "                            feat_text += f\"{key:30s}: {value}\\n\"\n",
    "                \n",
    "                self.feat_text.delete('1.0', tk.END)\n",
    "                self.feat_text.insert('1.0', feat_text)\n",
    "            \n",
    "            self.status_var.set(f\"Prediction complete: {classes[pred_class]}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.status_var.set(f\"Error: {e}\")\n",
    "            messagebox.showerror(\"Error\", str(e))\n",
    "    \n",
    "    def batch_predict(self):\n",
    "        if self.model is None:\n",
    "            messagebox.showerror(\"Error\", \"No model loaded!\")\n",
    "            return\n",
    "        \n",
    "        folder_path = filedialog.askdirectory(title=\"Select folder with images\")\n",
    "        if not folder_path:\n",
    "            return\n",
    "        \n",
    "        messagebox.showinfo(\"Info\", \"Batch prediction feature - coming soon!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        import multiprocessing as mp\n",
    "        mp.set_start_method('spawn', force=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"Professional Plantar Pressure Analysis System\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Device: {Config.DEVICE}\")\n",
    "    print(f\"Dataset: {Config.DATASET_DIR}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    Config.create_directories()\n",
    "    \n",
    "    root = tk.Tk()\n",
    "    app = ProfessionalPlantarApp(root)\n",
    "    root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
